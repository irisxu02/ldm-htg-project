{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c49434d-15c8-4a2c-82bf-eca4162aa5e8",
   "metadata": {},
   "source": [
    "# Implementation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d573b0c-6409-4602-b476-b99e59f6bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataset import IAMLinesDataset, IAMLinesDatasetPyTorch\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e90d79-8790-4f99-bdcd-026e801d4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Dataset parameters\n",
    "        self.img_size = (64, 256)  # Height, Width - Adjusted for line-level segments\n",
    "        self.batch_size = 64\n",
    "        self.num_workers = 4\n",
    "        \n",
    "        # VAE parameters\n",
    "        self.latent_dim = 512  # Latent dimension for the VAE\n",
    "        self.vae_lr = 1e-4\n",
    "        self.vae_epochs = 5 #orig: 50\n",
    "        \n",
    "        # Diffusion model parameters\n",
    "        self.timesteps = 1000  # Number of diffusion steps\n",
    "        self.beta_start = 1e-4  # Starting noise schedule value\n",
    "        self.beta_end = 2e-2  # Ending noise schedule value\n",
    "        self.diffusion_lr = 1e-4\n",
    "        self.diffusion_epochs = 10 #orig: 100\n",
    "        \n",
    "        # Style and content encoder parameters\n",
    "        self.style_dim = 256\n",
    "        self.content_dim = 256\n",
    "        \n",
    "        # Training parameters\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "            print(\"Using mac gpu\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            print(\"Using cuda gpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Using cpu\")\n",
    "        self.save_dir = \"./models\"\n",
    "        self.results_dir = \"./results\"\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76bdaf49-cc48-4cc7-9909-4d4eb4fe1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and preprocessing\n",
    "class IAMDatasetWrapper:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(config.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Load the IAM dataset and create train/val/test splits\"\"\"\n",
    "        print(\"Loading IAM dataset...\")\n",
    "        # Assuming IAMLinesDatasetPyTorch is already implemented\n",
    "        data_path = 'data/lines.tgz' \n",
    "        xml_path = 'data/xml.tgz'    \n",
    "        iam_dataset = IAMLinesDataset(data_path, xml_path)\n",
    "        full_dataset = IAMLinesDatasetPyTorch(iam_dataset=iam_dataset, transform=self.transform)\n",
    "        \n",
    "        # Split into train/val/test\n",
    "        train_size = int(0.8 * len(full_dataset))\n",
    "        val_size = int(0.1 * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded. Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c105ea-c9f4-4fc5-a9c3-9ff51d300b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VAE, self).__init__()\n",
    "        self.config = config\n",
    "        self.latent_dim = config.latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the encoder output\n",
    "        h, w = config.img_size\n",
    "        self.encoder_output_size = (h // 16, w // 16, 256)\n",
    "        self.encoder_flattened_dim = self.encoder_output_size[0] * self.encoder_output_size[1] * self.encoder_output_size[2]\n",
    "        \n",
    "        # Mean and log variance layers\n",
    "        self.fc_mu = nn.Linear(self.encoder_flattened_dim, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.encoder_flattened_dim, self.latent_dim)\n",
    "        \n",
    "        # Decoder input layer\n",
    "        self.decoder_input = nn.Linear(self.latent_dim, self.encoder_flattened_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output in range [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1, 256, self.encoder_output_size[0], self.encoder_output_size[1])\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Style Encoder\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(StyleEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # Projection head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, config.style_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        style = self.fc(x)\n",
    "        return style\n",
    "\n",
    "# Content Encoder (for text)\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, config, vocab_size=128, embedding_dim=128):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Projection head\n",
    "        self.fc = nn.Linear(512, config.content_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Concatenate forward and backward hidden states\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Project to content dimension\n",
    "        content = self.fc(hidden)\n",
    "        return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a9f882-b10b-4c22-81e3-9b6d3a18c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionUNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DiffusionUNet, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Calculate latent image dimensions\n",
    "        h, w = config.img_size\n",
    "        self.latent_h, self.latent_w = h // 16, w // 16  # 4×16 for 64×256 input\n",
    "        \n",
    "        # VAE bottleneck channels\n",
    "        vae_latent_channels = 256  # Matches VAE's spatial channels\n",
    "        \n",
    "        # Conditioning embedding dimension\n",
    "        cond_channels = 128\n",
    "        \n",
    "        # Time and conditioning embeddings\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "        self.style_embed = nn.Linear(config.style_dim, 128)\n",
    "        self.content_embed = nn.Linear(config.content_dim, 128)\n",
    "        \n",
    "        # Initial convolution - combines latent and conditioning\n",
    "        initial_in_channels = vae_latent_channels + cond_channels\n",
    "        initial_out_channels = 256\n",
    "        self.initial_conv = nn.Conv2d(initial_in_channels, initial_out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Adjusted down blocks - only use 2 instead of 3 due to small spatial dimensions\n",
    "        self.down1 = self._make_down_block(initial_out_channels, 256)  # 4×16 -> 2×8\n",
    "        self.down2 = self._make_down_block(256, 512)                  # 2×8 -> 1×4\n",
    "        \n",
    "        # Middle block\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        \n",
    "        # Adjusted up blocks\n",
    "        self.up1 = self._make_up_block(512, 256)                      # 1×4 -> 2×8 \n",
    "        self.up2 = self._make_up_block(256 + 256, 256)                # 2×8 -> 4×16\n",
    "        \n",
    "        # Output projection\n",
    "        self.out = nn.Conv2d(256, vae_latent_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def _make_down_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "    \n",
    "    def _make_up_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t, style, content):\n",
    "        # Input: [B, 256, 4, 16]\n",
    "        t = t.unsqueeze(-1).float()  # [B, 1]\n",
    "        \n",
    "        # Embeddings\n",
    "        t_emb = self.time_embed(t)  # [B, 128]\n",
    "        style_emb = self.style_embed(style)  # [B, 128]\n",
    "        content_emb = self.content_embed(content)  # [B, 128]\n",
    "        \n",
    "        # Combine and expand to spatial dimensions\n",
    "        cond_emb = (t_emb + style_emb + content_emb).unsqueeze(-1).unsqueeze(-1)  # [B, 128, 1, 1]\n",
    "        cond_emb = cond_emb.expand(-1, -1, self.latent_h, self.latent_w)  # [B, 128, 4, 16]\n",
    "        \n",
    "        # Concatenate with input\n",
    "        x_cat = torch.cat([x, cond_emb], dim=1)  # [B, 384, 4, 16]\n",
    "        \n",
    "        # Initial convolution\n",
    "        h = self.initial_conv(x_cat)  # [B, 256, 4, 16]\n",
    "        \n",
    "        # Encoder path - only 2 down blocks\n",
    "        d1 = self.down1(h)  # [B, 256, 2, 8]\n",
    "        d2 = self.down2(d1)  # [B, 512, 1, 4]\n",
    "        \n",
    "        # Middle\n",
    "        mid = self.mid(d2)  # [B, 512, 1, 4]\n",
    "        \n",
    "        # Decoder path - adjusted for 2 down blocks\n",
    "        u1 = self.up1(mid)  # [B, 256, 2, 8]\n",
    "        \n",
    "        # Skip connection with d1\n",
    "        u2_input = torch.cat([u1, d1], dim=1)  # [B, 512, 2, 8]\n",
    "        u2 = self.up2(u2_input)  # [B, 256, 4, 16]\n",
    "        \n",
    "        # Output - predict noise\n",
    "        return self.out(u2)  # [B, 256, 4, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1716214d-5552-4339-acba-351223c5e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusionModel:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Define beta schedule\n",
    "        self.beta = torch.linspace(config.beta_start, config.beta_end, config.timesteps).to(self.device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.vae = VAE(config).to(self.device)\n",
    "        self.style_encoder = StyleEncoder(config).to(self.device)\n",
    "        self.content_encoder = ContentEncoder(config).to(self.device)\n",
    "        self.diffusion_model = DiffusionUNet(config).to(self.device)\n",
    "        \n",
    "        # Define optimizers\n",
    "        self.vae_optimizer = optim.Adam(self.vae.parameters(), lr=config.vae_lr)\n",
    "        self.style_optimizer = optim.Adam(self.style_encoder.parameters(), lr=config.diffusion_lr)\n",
    "        self.content_optimizer = optim.Adam(self.content_encoder.parameters(), lr=config.diffusion_lr)\n",
    "        self.diffusion_optimizer = optim.Adam(self.diffusion_model.parameters(), lr=config.diffusion_lr)\n",
    "    \n",
    "    def train_vae(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Train the VAE model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.vae_epochs\n",
    "        \n",
    "        print(\"Training VAE...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.vae.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                self.vae_optimizer.zero_grad()\n",
    "                \n",
    "                # Get handwriting images\n",
    "                images = batch['image'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "                \n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = recon_loss + 0.001 * kl_loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.vae_optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self._validate_vae(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self._save_checkpoint(f\"vae_epoch_{epoch+1}.pt\", model=self.vae)\n",
    "        \n",
    "        print(\"VAE training completed.\")\n",
    "    \n",
    "    def _validate_vae(self, val_loader):\n",
    "        \"\"\"Validate the VAE model\"\"\"\n",
    "        self.vae.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "                \n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = recon_loss + 0.001 * kl_loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        return val_loss / len(val_loader)\n",
    "    \n",
    "    def train_diffusion(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Train the diffusion model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.diffusion_epochs\n",
    "        \n",
    "        print(\"Training Diffusion Model...\")\n",
    "        \n",
    "        # Ensure VAE is in eval mode and frozen\n",
    "        self.vae.eval()\n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Calculate latent dimensions for clarity\n",
    "        latent_h = self.config.img_size[0] // 16  # Height divided by total downsampling factor\n",
    "        latent_w = self.config.img_size[1] // 16  # Width divided by total downsampling factor\n",
    "        latent_channels = 256  # Number of channels in the VAE bottleneck\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.diffusion_model.train()\n",
    "            self.style_encoder.train()\n",
    "            self.content_encoder.train()\n",
    "            \n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                # Zero gradients\n",
    "                self.diffusion_optimizer.zero_grad()\n",
    "                self.style_optimizer.zero_grad()\n",
    "                self.content_optimizer.zero_grad()\n",
    "                \n",
    "                # Get data\n",
    "                images = batch['image'].to(self.device)\n",
    "                text = batch['transcription']\n",
    "                \n",
    "                # Process text data - create proper tokenization\n",
    "                # For simplicity, using a fixed length and padding\n",
    "                max_seq_len = 100\n",
    "                text_indices = torch.zeros(len(text), max_seq_len).long().to(self.device)\n",
    "                for i, t in enumerate(text):\n",
    "                    # Convert string to character indices (simplified)\n",
    "                    # In production, use a proper tokenizer\n",
    "                    for j, char in enumerate(t[:max_seq_len]):\n",
    "                        text_indices[i, j] = ord(char) % 128  # Simple char to index mapping\n",
    "                \n",
    "                # Extract spatial latent representations from the VAE\n",
    "                with torch.no_grad():\n",
    "                    # Get encoder outputs\n",
    "                    encoder_output = self.vae.encoder(images)\n",
    "                    # Flatten for mu/logvar calculation\n",
    "                    encoder_flat = encoder_output.view(images.shape[0], -1)\n",
    "                    # Get latent distribution parameters\n",
    "                    mu = self.vae.fc_mu(encoder_flat)\n",
    "                    logvar = self.vae.fc_logvar(encoder_flat)\n",
    "                    # Sample from distribution\n",
    "                    z_flat = self.vae.reparameterize(mu, logvar)\n",
    "                    \n",
    "                    # Map back to spatial representation\n",
    "                    z_projected = self.vae.decoder_input(z_flat)\n",
    "                    # Reshape to spatial dimensions expected by diffusion model\n",
    "                    # [batch_size, channels, height, width]\n",
    "                    latent_spatial = z_projected.view(\n",
    "                        images.shape[0],\n",
    "                        latent_channels,\n",
    "                        latent_h,\n",
    "                        latent_w\n",
    "                    )\n",
    "                \n",
    "                # Sample random timesteps for each image\n",
    "                t = torch.randint(0, self.config.timesteps, (images.shape[0],), device=self.device)\n",
    "                \n",
    "                # Add noise to latent images\n",
    "                noise = torch.randn_like(latent_spatial)\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "                \n",
    "                # Calculate noisy latents\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                \n",
    "                # Encode style and content\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "                # Predict noise using the diffusion UNet\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "                # Compute loss between predicted and actual noise\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Apply gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.diffusion_model.parameters(), max_norm=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.style_encoder.parameters(), max_norm=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.content_encoder.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.diffusion_optimizer.step()\n",
    "                self.style_optimizer.step()\n",
    "                self.content_optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Calculate average training loss\n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation after each epoch\n",
    "            val_loss = self._validate_diffusion(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:  # Save more frequently\n",
    "                self._save_checkpoint(\n",
    "                    f\"diffusion_epoch_{epoch+1}.pt\", \n",
    "                    models=[self.diffusion_model, self.style_encoder, self.content_encoder]\n",
    "                )\n",
    "        \n",
    "        print(\"Diffusion model training completed!\")\n",
    "    \n",
    "    def _validate_diffusion(self, val_loader):\n",
    "        \"\"\"Validate the diffusion model\"\"\"\n",
    "        # Set models to evaluation mode\n",
    "        self.diffusion_model.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Calculate latent dimensions\n",
    "        latent_h = self.config.img_size[0] // 16\n",
    "        latent_w = self.config.img_size[1] // 16\n",
    "        latent_channels = 256\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Get data\n",
    "                images = batch['image'].to(self.device)\n",
    "                text = batch['transcription']\n",
    "                \n",
    "                # Process text data\n",
    "                max_seq_len = 100\n",
    "                text_indices = torch.zeros(len(text), max_seq_len).long().to(self.device)\n",
    "                for i, t in enumerate(text):\n",
    "                    for j, char in enumerate(t[:max_seq_len]):\n",
    "                        text_indices[i, j] = ord(char) % 128\n",
    "                \n",
    "                # Extract spatial latents from VAE\n",
    "                encoder_output = self.vae.encoder(images)\n",
    "                encoder_flat = encoder_output.view(images.shape[0], -1)\n",
    "                mu = self.vae.fc_mu(encoder_flat)\n",
    "                logvar = self.vae.fc_logvar(encoder_flat)\n",
    "                z_flat = self.vae.reparameterize(mu, logvar)\n",
    "                \n",
    "                # Map to spatial representation\n",
    "                z_projected = self.vae.decoder_input(z_flat)\n",
    "                latent_spatial = z_projected.view(\n",
    "                    images.shape[0], \n",
    "                    latent_channels,\n",
    "                    latent_h,\n",
    "                    latent_w\n",
    "                )\n",
    "                \n",
    "                # Sample random timesteps\n",
    "                t = torch.randint(0, self.config.timesteps, (images.shape[0],), device=self.device)\n",
    "                \n",
    "                # Add noise to latents\n",
    "                noise = torch.randn_like(latent_spatial)\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                \n",
    "                # Get style and content features\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        return val_loss / num_batches\n",
    "    \n",
    "    def _save_checkpoint(self, filename, model=None, models=None):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        save_path = os.path.join(self.config.save_dir, filename)\n",
    "        \n",
    "        if model is not None:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        elif models is not None:\n",
    "            save_dict = {}\n",
    "            for i, m in enumerate(models):\n",
    "                save_dict[f\"model_{i}\"] = m.state_dict()\n",
    "            torch.save(save_dict, save_path)\n",
    "    \n",
    "    def generate_handwriting(self, style_image, text, steps=None):\n",
    "        \"\"\"Generate handwriting with a given style and text\"\"\"\n",
    "        if steps is None:\n",
    "            steps = self.config.timesteps // 10  # Using fewer steps for inference can be faster\n",
    "        \n",
    "        # Ensure models are in eval mode\n",
    "        self.vae.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        self.diffusion_model.eval()\n",
    "        \n",
    "        # Calculate latent dimensions\n",
    "        latent_h = self.config.img_size[0] // 16\n",
    "        latent_w = self.config.img_size[1] // 16\n",
    "        latent_channels = 256\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Preprocess style image - make sure it has batch dimension\n",
    "            if isinstance(style_image, torch.Tensor):\n",
    "                # Add batch dimension if needed\n",
    "                if style_image.dim() == 3:\n",
    "                    style_image = style_image.unsqueeze(0)\n",
    "            else:\n",
    "                # Convert to tensor if needed (e.g., if it's a PIL image)\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize(self.config.img_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,))\n",
    "                ])\n",
    "                style_image = transform(style_image).unsqueeze(0)\n",
    "            \n",
    "            style_image = style_image.to(self.device)\n",
    "            \n",
    "            # Encode style - now expecting batch dimension correctly\n",
    "            style_features = self.style_encoder(style_image)\n",
    "            \n",
    "            # Process text input - always ensure batch dimension of 1\n",
    "            max_seq_len = 100\n",
    "            text_indices = torch.zeros(1, max_seq_len).long().to(self.device)\n",
    "            for i, char in enumerate(text[:max_seq_len]):\n",
    "                text_indices[0, i] = ord(char) % 128\n",
    "            \n",
    "            # Get content features\n",
    "            content_features = self.content_encoder(text_indices)\n",
    "            \n",
    "            # Start with random noise in the latent space (spatial format)\n",
    "            latent = torch.randn(1, latent_channels, latent_h, latent_w).to(self.device)\n",
    "            \n",
    "            # Reverse diffusion process\n",
    "            for i in tqdm(range(steps), desc=\"Generating handwriting\"):\n",
    "                # Get the timestep (counting backwards)\n",
    "                t = self.config.timesteps - i - 1\n",
    "                timestep = torch.tensor([t], device=self.device)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = self.diffusion_model(latent, timestep, style_features, content_features)\n",
    "                \n",
    "                # Get alpha values for current timestep\n",
    "                alpha = self.alpha[t]\n",
    "                alpha_cumprod = self.alpha_cumprod[t]\n",
    "                beta = self.beta[t]\n",
    "                \n",
    "                # No noise at timestep 0\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(latent)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(latent)\n",
    "                \n",
    "                # Update latent using the DDPM formula\n",
    "                latent = (1 / torch.sqrt(alpha)) * (\n",
    "                    latent - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * noise_pred\n",
    "                ) + torch.sqrt(beta) * noise\n",
    "            \n",
    "            # Reshape latent for the VAE decoder\n",
    "            # First reshape to appropriate format for VAE - THIS IS IMPORTANT\n",
    "            latent_flat = latent.reshape(1, -1)\n",
    "            \n",
    "            # Make sure the latent dimension matches what VAE expects\n",
    "            # Use the decoder_input layer to map from flattened spatial features to VAE latent dim\n",
    "            latent_for_decoder = self.vae.decoder_input(latent_flat[:, :self.config.latent_dim])\n",
    "            \n",
    "            # Reshape back to spatial format for the decoder\n",
    "            latent_spatial = latent_for_decoder.view(\n",
    "                1, \n",
    "                256,  # Match VAE's expected channel dimension\n",
    "                latent_h, \n",
    "                latent_w\n",
    "            )\n",
    "            \n",
    "            # Pass through VAE decoder\n",
    "            generated_image = self.vae.decoder(latent_spatial)\n",
    "            \n",
    "            # Convert to range [0, 1] for visualization\n",
    "            generated_image = (generated_image + 1) / 2\n",
    "            \n",
    "            return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eea9d1c-92ad-4bbd-a349-5c07e8fc989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mac gpu\n"
     ]
    }
   ],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06baa2f5-1b48-4593-af90-338207dd6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrapper = IAMDatasetWrapper(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d74a560c-56ae-469f-8e19-614a1b1227df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IAM dataset...\n",
      "Dataset loaded. Train: 10682, Val: 1335, Test: 1336\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = data_wrapper.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "616186c4-312b-480e-9f90-329e92b47fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDiffusionModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753a9e77-c738-47e0-8017-8f8c0c7672bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:30<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.0998, Val Loss: 0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:31<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 0.0860, Val Loss: 0.0844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:31<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 0.0827, Val Loss: 0.0822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:31<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 0.0811, Val Loss: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:31<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 0.0802, Val Loss: 0.0805\n",
      "VAE training completed.\n",
      "Took 4.81 minutes to train\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "model.train_vae(train_loader, val_loader)\n",
    "print(f\"Took {round((time.time()-t1)/60, 2)} minutes to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed28b430-e407-4f7f-bd28-fba1d1e8df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Diffusion Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:54<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.0029, Val Loss: 1.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 1.0003, Val Loss: 1.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 1.0004, Val Loss: 1.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 1.0003, Val Loss: 1.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 1.0002, Val Loss: 1.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 1.0002, Val Loss: 1.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 1.0005, Val Loss: 1.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 1.0002, Val Loss: 1.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|█████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 1.0003, Val Loss: 0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 1.0003, Val Loss: 1.0005\n",
      "Diffusion model training completed!\n",
      "Took 13.8 minutes to train\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "model.train_diffusion(train_loader, val_loader) #mark-b\n",
    "print(f\"Took {round((time.time()-t1)/60, 2)} minutes to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79564054-92c7-427a-8c2f-76033fb0b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating handwriting: 100%|████████████████████████████████████████| 100/100 [00:00<00:00, 145.43it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, batch in enumerate(test_loader):\n",
    "    if i >= 1:  # Generate 5 samples\n",
    "        break\n",
    "    \n",
    "    # Get a sample for style reference\n",
    "    style_image = batch['image'][0]\n",
    "    text = batch['transcription'][0]\n",
    "    \n",
    "    # Generate new handwriting with the same style but different text\n",
    "    generated = model.generate_handwriting(style_image, \"This is a generated sample.\")\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(style_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Generated\")\n",
    "    plt.imshow(generated.squeeze().cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(config.results_dir, f\"sample_{i}.png\"))\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658fdc1-72e9-42c6-b0a0-84733c598638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
