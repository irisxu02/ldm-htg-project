{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1777cfdd-7302-4fb7-a653-9accc160d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4409897-bcec-4a00-a49d-ee7509b20d48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusion3Pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'diffusers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e68e910-e22e-4f24-9d17-5ac6267534c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032e8ea4c5864912a24af402ac3b4779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1c357938d84354ba3abe0e28d9c66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)\n",
    "# from https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_3\n",
    "model_id = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2116af8d-a188-4d15-9ee0-77596fb701a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23821.2096 MB allocated\n",
      "24192.745472 MB reserved\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated() / 1e6, \"MB allocated\")\n",
    "print(torch.cuda.memory_reserved() / 1e6, \"MB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6282875c-c6d8-4eae-9fa1-d038ebf87bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f157bc70-0a94-4140-b447-be91c65c1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = pipe.transformer\n",
    "vae = pipe.vae\n",
    "scheduler = pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7353a08-a178-4088-afe8-c7ae9a3eb97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiT: FrozenDict([('sample_size', 128), ('patch_size', 2), ('in_channels', 16), ('num_layers', 24), ('attention_head_dim', 64), ('num_attention_heads', 24), ('joint_attention_dim', 4096), ('caption_projection_dim', 1536), ('pooled_projection_dim', 2048), ('out_channels', 16), ('pos_embed_max_size', 192), ('dual_attention_layers', ()), ('qk_norm', None), ('_use_default_values', ['dual_attention_layers', 'qk_norm']), ('_class_name', 'SD3Transformer2DModel'), ('_diffusers_version', '0.29.0.dev0'), ('_name_or_path', '/home/irisx/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3-medium-diffusers/snapshots/ea42f8cef0f178587cf766dc8129abd379c90671/transformer')])\n",
      "VAE: FrozenDict([('in_channels', 3), ('out_channels', 3), ('down_block_types', ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D']), ('up_block_types', ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D']), ('block_out_channels', [128, 256, 512, 512]), ('layers_per_block', 2), ('act_fn', 'silu'), ('latent_channels', 16), ('norm_num_groups', 32), ('sample_size', 1024), ('scaling_factor', 1.5305), ('shift_factor', 0.0609), ('latents_mean', None), ('latents_std', None), ('force_upcast', True), ('use_quant_conv', False), ('use_post_quant_conv', False), ('mid_block_add_attention', True), ('_use_default_values', ['mid_block_add_attention']), ('_class_name', 'AutoencoderKL'), ('_diffusers_version', '0.29.0.dev0'), ('_name_or_path', '/home/irisx/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3-medium-diffusers/snapshots/ea42f8cef0f178587cf766dc8129abd379c90671/vae')])\n",
      "Scheduler: FlowMatchEulerDiscreteScheduler {\n",
      "  \"_class_name\": \"FlowMatchEulerDiscreteScheduler\",\n",
      "  \"_diffusers_version\": \"0.33.1\",\n",
      "  \"base_image_seq_len\": 256,\n",
      "  \"base_shift\": 0.5,\n",
      "  \"invert_sigmas\": false,\n",
      "  \"max_image_seq_len\": 4096,\n",
      "  \"max_shift\": 1.15,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"shift\": 3.0,\n",
      "  \"shift_terminal\": null,\n",
      "  \"time_shift_type\": \"exponential\",\n",
      "  \"use_beta_sigmas\": false,\n",
      "  \"use_dynamic_shifting\": false,\n",
      "  \"use_exponential_sigmas\": false,\n",
      "  \"use_karras_sigmas\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"DiT:\", transformer.config)\n",
    "print(\"VAE:\", vae.config)\n",
    "print(\"Scheduler:\", scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b00a248-923a-4328-b71f-1cdc4b4a6663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e700fda4f784d30aa7cabd815b8c6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498f3d26ba9b48ce8ac8fa523b3b6885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b64bca067147da9b4eddff0f427349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4f7416644441e29f1f6e9626fce939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/657 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_encoder = CanineModel.from_pretrained(\"google/canine-c\", torch_dtype=torch.float16)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"google/canine-c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32014be9-a129-4358-b521-8dcbb3837418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Encoder: CanineConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"CanineModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 57344,\n",
      "  \"downsampling_rate\": 4,\n",
      "  \"eos_token_id\": 57345,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"local_transformer_stride\": 128,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"canine\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hash_buckets\": 16384,\n",
      "  \"num_hash_functions\": 8,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"upsampling_kernel_size\": 4,\n",
      "  \"use_cache\": true\n",
      "}\n",
      "\n",
      "Text Tokenizer: CanineTokenizer(name_or_path='google/canine-c', vocab_size=1114112, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '\\ue000', 'eos_token': '\\ue001', 'sep_token': '\\ue001', 'pad_token': '\\x00', 'cls_token': '\\ue000', 'mask_token': '\\ue003'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"\u0000\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t57344: AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t57345: AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t57347: AddedToken(\"\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Text Encoder:\", text_encoder.config)\n",
    "print(\"Text Tokenizer:\", text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39201d6d-528b-4180-bd1d-b9f910524761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da60e882-c6f3-4eb0-8a4f-316ea190518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditioner(nn.Module):\n",
    "    def __init__(self, text_dim=768, style_dim=256, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(text_dim + style_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)  # Project to DiT's cross-attn dimension\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_embeds, style_embeds):\n",
    "        combined = torch.cat([text_embeds, style_embeds], dim=-1)\n",
    "        return self.proj(combined)  # [batch, seq_len, hidden_dim]\n",
    "\n",
    "conditioner = Conditioner()#.to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fbdd794-cf21-4d05-807a-ecfe620aa0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioner(\n",
      "  (proj): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(conditioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7c6006a-5548-45ba-aef6-4f1711beb760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.conv_stem.weight', 'model.bn1.weight', 'model.bn1.bias', 'model.bn1.running_mean', 'model.bn1.running_var', 'model.bn1.num_batches_tracked', 'model.blocks.0.0.conv_dw.weight', 'model.blocks.0.0.bn1.weight', 'model.blocks.0.0.bn1.bias', 'model.blocks.0.0.bn1.running_mean', 'model.blocks.0.0.bn1.running_var', 'model.blocks.0.0.bn1.num_batches_tracked', 'model.blocks.0.0.conv_pw.weight', 'model.blocks.0.0.bn2.weight', 'model.blocks.0.0.bn2.bias', 'model.blocks.0.0.bn2.running_mean', 'model.blocks.0.0.bn2.running_var', 'model.blocks.0.0.bn2.num_batches_tracked', 'model.blocks.1.0.conv_pw.weight', 'model.blocks.1.0.bn1.weight', 'model.blocks.1.0.bn1.bias', 'model.blocks.1.0.bn1.running_mean', 'model.blocks.1.0.bn1.running_var', 'model.blocks.1.0.bn1.num_batches_tracked', 'model.blocks.1.0.conv_dw.weight', 'model.blocks.1.0.bn2.weight', 'model.blocks.1.0.bn2.bias', 'model.blocks.1.0.bn2.running_mean', 'model.blocks.1.0.bn2.running_var', 'model.blocks.1.0.bn2.num_batches_tracked', 'model.blocks.1.0.conv_pwl.weight', 'model.blocks.1.0.bn3.weight', 'model.blocks.1.0.bn3.bias', 'model.blocks.1.0.bn3.running_mean', 'model.blocks.1.0.bn3.running_var', 'model.blocks.1.0.bn3.num_batches_tracked', 'model.blocks.1.1.conv_pw.weight', 'model.blocks.1.1.bn1.weight', 'model.blocks.1.1.bn1.bias', 'model.blocks.1.1.bn1.running_mean', 'model.blocks.1.1.bn1.running_var', 'model.blocks.1.1.bn1.num_batches_tracked', 'model.blocks.1.1.conv_dw.weight', 'model.blocks.1.1.bn2.weight', 'model.blocks.1.1.bn2.bias', 'model.blocks.1.1.bn2.running_mean', 'model.blocks.1.1.bn2.running_var', 'model.blocks.1.1.bn2.num_batches_tracked', 'model.blocks.1.1.conv_pwl.weight', 'model.blocks.1.1.bn3.weight', 'model.blocks.1.1.bn3.bias', 'model.blocks.1.1.bn3.running_mean', 'model.blocks.1.1.bn3.running_var', 'model.blocks.1.1.bn3.num_batches_tracked', 'model.blocks.2.0.conv_pw.weight', 'model.blocks.2.0.bn1.weight', 'model.blocks.2.0.bn1.bias', 'model.blocks.2.0.bn1.running_mean', 'model.blocks.2.0.bn1.running_var', 'model.blocks.2.0.bn1.num_batches_tracked', 'model.blocks.2.0.conv_dw.weight', 'model.blocks.2.0.bn2.weight', 'model.blocks.2.0.bn2.bias', 'model.blocks.2.0.bn2.running_mean', 'model.blocks.2.0.bn2.running_var', 'model.blocks.2.0.bn2.num_batches_tracked', 'model.blocks.2.0.conv_pwl.weight', 'model.blocks.2.0.bn3.weight', 'model.blocks.2.0.bn3.bias', 'model.blocks.2.0.bn3.running_mean', 'model.blocks.2.0.bn3.running_var', 'model.blocks.2.0.bn3.num_batches_tracked', 'model.blocks.2.1.conv_pw.weight', 'model.blocks.2.1.bn1.weight', 'model.blocks.2.1.bn1.bias', 'model.blocks.2.1.bn1.running_mean', 'model.blocks.2.1.bn1.running_var', 'model.blocks.2.1.bn1.num_batches_tracked', 'model.blocks.2.1.conv_dw.weight', 'model.blocks.2.1.bn2.weight', 'model.blocks.2.1.bn2.bias', 'model.blocks.2.1.bn2.running_mean', 'model.blocks.2.1.bn2.running_var', 'model.blocks.2.1.bn2.num_batches_tracked', 'model.blocks.2.1.conv_pwl.weight', 'model.blocks.2.1.bn3.weight', 'model.blocks.2.1.bn3.bias', 'model.blocks.2.1.bn3.running_mean', 'model.blocks.2.1.bn3.running_var', 'model.blocks.2.1.bn3.num_batches_tracked', 'model.blocks.2.2.conv_pw.weight', 'model.blocks.2.2.bn1.weight', 'model.blocks.2.2.bn1.bias', 'model.blocks.2.2.bn1.running_mean', 'model.blocks.2.2.bn1.running_var', 'model.blocks.2.2.bn1.num_batches_tracked', 'model.blocks.2.2.conv_dw.weight', 'model.blocks.2.2.bn2.weight', 'model.blocks.2.2.bn2.bias', 'model.blocks.2.2.bn2.running_mean', 'model.blocks.2.2.bn2.running_var', 'model.blocks.2.2.bn2.num_batches_tracked', 'model.blocks.2.2.conv_pwl.weight', 'model.blocks.2.2.bn3.weight', 'model.blocks.2.2.bn3.bias', 'model.blocks.2.2.bn3.running_mean', 'model.blocks.2.2.bn3.running_var', 'model.blocks.2.2.bn3.num_batches_tracked', 'model.blocks.3.0.conv_pw.weight', 'model.blocks.3.0.bn1.weight', 'model.blocks.3.0.bn1.bias', 'model.blocks.3.0.bn1.running_mean', 'model.blocks.3.0.bn1.running_var', 'model.blocks.3.0.bn1.num_batches_tracked', 'model.blocks.3.0.conv_dw.weight', 'model.blocks.3.0.bn2.weight', 'model.blocks.3.0.bn2.bias', 'model.blocks.3.0.bn2.running_mean', 'model.blocks.3.0.bn2.running_var', 'model.blocks.3.0.bn2.num_batches_tracked', 'model.blocks.3.0.conv_pwl.weight', 'model.blocks.3.0.bn3.weight', 'model.blocks.3.0.bn3.bias', 'model.blocks.3.0.bn3.running_mean', 'model.blocks.3.0.bn3.running_var', 'model.blocks.3.0.bn3.num_batches_tracked', 'model.blocks.3.1.conv_pw.weight', 'model.blocks.3.1.bn1.weight', 'model.blocks.3.1.bn1.bias', 'model.blocks.3.1.bn1.running_mean', 'model.blocks.3.1.bn1.running_var', 'model.blocks.3.1.bn1.num_batches_tracked', 'model.blocks.3.1.conv_dw.weight', 'model.blocks.3.1.bn2.weight', 'model.blocks.3.1.bn2.bias', 'model.blocks.3.1.bn2.running_mean', 'model.blocks.3.1.bn2.running_var', 'model.blocks.3.1.bn2.num_batches_tracked', 'model.blocks.3.1.conv_pwl.weight', 'model.blocks.3.1.bn3.weight', 'model.blocks.3.1.bn3.bias', 'model.blocks.3.1.bn3.running_mean', 'model.blocks.3.1.bn3.running_var', 'model.blocks.3.1.bn3.num_batches_tracked', 'model.blocks.3.2.conv_pw.weight', 'model.blocks.3.2.bn1.weight', 'model.blocks.3.2.bn1.bias', 'model.blocks.3.2.bn1.running_mean', 'model.blocks.3.2.bn1.running_var', 'model.blocks.3.2.bn1.num_batches_tracked', 'model.blocks.3.2.conv_dw.weight', 'model.blocks.3.2.bn2.weight', 'model.blocks.3.2.bn2.bias', 'model.blocks.3.2.bn2.running_mean', 'model.blocks.3.2.bn2.running_var', 'model.blocks.3.2.bn2.num_batches_tracked', 'model.blocks.3.2.conv_pwl.weight', 'model.blocks.3.2.bn3.weight', 'model.blocks.3.2.bn3.bias', 'model.blocks.3.2.bn3.running_mean', 'model.blocks.3.2.bn3.running_var', 'model.blocks.3.2.bn3.num_batches_tracked', 'model.blocks.3.3.conv_pw.weight', 'model.blocks.3.3.bn1.weight', 'model.blocks.3.3.bn1.bias', 'model.blocks.3.3.bn1.running_mean', 'model.blocks.3.3.bn1.running_var', 'model.blocks.3.3.bn1.num_batches_tracked', 'model.blocks.3.3.conv_dw.weight', 'model.blocks.3.3.bn2.weight', 'model.blocks.3.3.bn2.bias', 'model.blocks.3.3.bn2.running_mean', 'model.blocks.3.3.bn2.running_var', 'model.blocks.3.3.bn2.num_batches_tracked', 'model.blocks.3.3.conv_pwl.weight', 'model.blocks.3.3.bn3.weight', 'model.blocks.3.3.bn3.bias', 'model.blocks.3.3.bn3.running_mean', 'model.blocks.3.3.bn3.running_var', 'model.blocks.3.3.bn3.num_batches_tracked', 'model.blocks.4.0.conv_pw.weight', 'model.blocks.4.0.bn1.weight', 'model.blocks.4.0.bn1.bias', 'model.blocks.4.0.bn1.running_mean', 'model.blocks.4.0.bn1.running_var', 'model.blocks.4.0.bn1.num_batches_tracked', 'model.blocks.4.0.conv_dw.weight', 'model.blocks.4.0.bn2.weight', 'model.blocks.4.0.bn2.bias', 'model.blocks.4.0.bn2.running_mean', 'model.blocks.4.0.bn2.running_var', 'model.blocks.4.0.bn2.num_batches_tracked', 'model.blocks.4.0.conv_pwl.weight', 'model.blocks.4.0.bn3.weight', 'model.blocks.4.0.bn3.bias', 'model.blocks.4.0.bn3.running_mean', 'model.blocks.4.0.bn3.running_var', 'model.blocks.4.0.bn3.num_batches_tracked', 'model.blocks.4.1.conv_pw.weight', 'model.blocks.4.1.bn1.weight', 'model.blocks.4.1.bn1.bias', 'model.blocks.4.1.bn1.running_mean', 'model.blocks.4.1.bn1.running_var', 'model.blocks.4.1.bn1.num_batches_tracked', 'model.blocks.4.1.conv_dw.weight', 'model.blocks.4.1.bn2.weight', 'model.blocks.4.1.bn2.bias', 'model.blocks.4.1.bn2.running_mean', 'model.blocks.4.1.bn2.running_var', 'model.blocks.4.1.bn2.num_batches_tracked', 'model.blocks.4.1.conv_pwl.weight', 'model.blocks.4.1.bn3.weight', 'model.blocks.4.1.bn3.bias', 'model.blocks.4.1.bn3.running_mean', 'model.blocks.4.1.bn3.running_var', 'model.blocks.4.1.bn3.num_batches_tracked', 'model.blocks.4.2.conv_pw.weight', 'model.blocks.4.2.bn1.weight', 'model.blocks.4.2.bn1.bias', 'model.blocks.4.2.bn1.running_mean', 'model.blocks.4.2.bn1.running_var', 'model.blocks.4.2.bn1.num_batches_tracked', 'model.blocks.4.2.conv_dw.weight', 'model.blocks.4.2.bn2.weight', 'model.blocks.4.2.bn2.bias', 'model.blocks.4.2.bn2.running_mean', 'model.blocks.4.2.bn2.running_var', 'model.blocks.4.2.bn2.num_batches_tracked', 'model.blocks.4.2.conv_pwl.weight', 'model.blocks.4.2.bn3.weight', 'model.blocks.4.2.bn3.bias', 'model.blocks.4.2.bn3.running_mean', 'model.blocks.4.2.bn3.running_var', 'model.blocks.4.2.bn3.num_batches_tracked', 'model.blocks.5.0.conv_pw.weight', 'model.blocks.5.0.bn1.weight', 'model.blocks.5.0.bn1.bias', 'model.blocks.5.0.bn1.running_mean', 'model.blocks.5.0.bn1.running_var', 'model.blocks.5.0.bn1.num_batches_tracked', 'model.blocks.5.0.conv_dw.weight', 'model.blocks.5.0.bn2.weight', 'model.blocks.5.0.bn2.bias', 'model.blocks.5.0.bn2.running_mean', 'model.blocks.5.0.bn2.running_var', 'model.blocks.5.0.bn2.num_batches_tracked', 'model.blocks.5.0.conv_pwl.weight', 'model.blocks.5.0.bn3.weight', 'model.blocks.5.0.bn3.bias', 'model.blocks.5.0.bn3.running_mean', 'model.blocks.5.0.bn3.running_var', 'model.blocks.5.0.bn3.num_batches_tracked', 'model.blocks.5.1.conv_pw.weight', 'model.blocks.5.1.bn1.weight', 'model.blocks.5.1.bn1.bias', 'model.blocks.5.1.bn1.running_mean', 'model.blocks.5.1.bn1.running_var', 'model.blocks.5.1.bn1.num_batches_tracked', 'model.blocks.5.1.conv_dw.weight', 'model.blocks.5.1.bn2.weight', 'model.blocks.5.1.bn2.bias', 'model.blocks.5.1.bn2.running_mean', 'model.blocks.5.1.bn2.running_var', 'model.blocks.5.1.bn2.num_batches_tracked', 'model.blocks.5.1.conv_pwl.weight', 'model.blocks.5.1.bn3.weight', 'model.blocks.5.1.bn3.bias', 'model.blocks.5.1.bn3.running_mean', 'model.blocks.5.1.bn3.running_var', 'model.blocks.5.1.bn3.num_batches_tracked', 'model.blocks.5.2.conv_pw.weight', 'model.blocks.5.2.bn1.weight', 'model.blocks.5.2.bn1.bias', 'model.blocks.5.2.bn1.running_mean', 'model.blocks.5.2.bn1.running_var', 'model.blocks.5.2.bn1.num_batches_tracked', 'model.blocks.5.2.conv_dw.weight', 'model.blocks.5.2.bn2.weight', 'model.blocks.5.2.bn2.bias', 'model.blocks.5.2.bn2.running_mean', 'model.blocks.5.2.bn2.running_var', 'model.blocks.5.2.bn2.num_batches_tracked', 'model.blocks.5.2.conv_pwl.weight', 'model.blocks.5.2.bn3.weight', 'model.blocks.5.2.bn3.bias', 'model.blocks.5.2.bn3.running_mean', 'model.blocks.5.2.bn3.running_var', 'model.blocks.5.2.bn3.num_batches_tracked', 'model.blocks.6.0.conv_pw.weight', 'model.blocks.6.0.bn1.weight', 'model.blocks.6.0.bn1.bias', 'model.blocks.6.0.bn1.running_mean', 'model.blocks.6.0.bn1.running_var', 'model.blocks.6.0.bn1.num_batches_tracked', 'model.blocks.6.0.conv_dw.weight', 'model.blocks.6.0.bn2.weight', 'model.blocks.6.0.bn2.bias', 'model.blocks.6.0.bn2.running_mean', 'model.blocks.6.0.bn2.running_var', 'model.blocks.6.0.bn2.num_batches_tracked', 'model.blocks.6.0.conv_pwl.weight', 'model.blocks.6.0.bn3.weight', 'model.blocks.6.0.bn3.bias', 'model.blocks.6.0.bn3.running_mean', 'model.blocks.6.0.bn3.running_var', 'model.blocks.6.0.bn3.num_batches_tracked', 'model.conv_head.weight', 'model.bn2.weight', 'model.bn2.bias', 'model.bn2.running_mean', 'model.bn2.running_var', 'model.bn2.num_batches_tracked', 'classifier.weight', 'classifier.bias'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'odict_keys'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m pretrained_weights = torch.load(\u001b[33m\"\u001b[39m\u001b[33mDiffusionPen/style_models/iam_style_diffusionpen.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=\u001b[33m\"\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(pretrained_weights.keys())\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mstyle_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_weights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Move to GPU and FP16\u001b[39;00m\n\u001b[32m     23\u001b[39m style_encoder = style_encoder.half()\u001b[38;5;66;03m#.to(\"cuda\").half()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/envs/440/lib/python3.11/site-packages/torch/nn/modules/module.py:2525\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[32m   2489\u001b[39m \n\u001b[32m   2490\u001b[39m \u001b[33;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2522\u001b[39m \u001b[33;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[32m   2523\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[32m-> \u001b[39m\u001b[32m2525\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   2526\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2527\u001b[39m     )\n\u001b[32m   2529\u001b[39m missing_keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m   2530\u001b[39m unexpected_keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n",
      "\u001b[31mTypeError\u001b[39m: Expected state_dict to be dict-like, got <class 'odict_keys'>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, style_embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.base = mobilenet_v3_small(pretrained=False)  # Match DiffusionPen's backbone\n",
    "        self.base.classifier = nn.Sequential(\n",
    "            nn.Linear(576, style_embed_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.base(image)\n",
    "\n",
    "style_encoder = StyleEncoder()\n",
    "pretrained_weights = torch.load(\"DiffusionPen/style_models/iam_style_diffusionpen.pth\", map_location=\"cuda:0\")\n",
    "style_encoder.load_state_dict(pretrained_weights.keys())\n",
    "\n",
    "# Move to GPU and FP16\n",
    "style_encoder = style_encoder.half()#.to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89c93d-c4c5-46aa-82b3-9f137a15bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "class HandwritingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        self.data = load_dataset(\"your_dataset_name\", split=split)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"image\": item[\"image\"],  # Preprocess to [3, H, W]\n",
    "            \"text\": item[\"text\"],\n",
    "            \"writer_id\": item[\"writer_id\"] #style\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Create dataloader\n",
    "dataset = HandwritingDataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2542339-1654-4f54-b014-9dc980698c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(style_encoder.parameters()) + list(conditioner.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Training\n",
    "for epoch in range(100):\n",
    "    for batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        images = batch[\"image\"].to(\"cuda\", dtype=torch.float16)\n",
    "        texts = batch[\"text\"]\n",
    "        writer_ids = batch[\"writer_id\"]\n",
    "        \n",
    "        # Encode text\n",
    "        text_inputs = text_tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "        text_embeds = text_encoder(**text_inputs).last_hidden_state  # [batch, seq_len, 768]\n",
    "        \n",
    "        # Encode style\n",
    "        style_embeds = style_encoder(images)  # [batch, 256]\n",
    "        \n",
    "        # Combine with conditioner\n",
    "        cond_embeds = conditioner(text_embeds, style_embeds.unsqueeze(1))  # [batch, seq_len, 1024]\n",
    "        \n",
    "        # VAE Encode\n",
    "        latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (latents.shape[0],))\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # Predict noise (DiT forward)\n",
    "        noise_pred = transformer(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states=cond_embeds  # Our fused conditioning!\n",
    "        ).sample\n",
    "        \n",
    "        # Loss\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd043b0c-1cf4-4129-9c7d-27f05c362854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_handwriting(text, style_image, num_inference_steps=20):\n",
    "    # Encode style\n",
    "    style_emb = style_encoder(style_image.unsqueeze(0).to(\"cuda\").half())\n",
    "    \n",
    "    # Encode text\n",
    "    text_inputs = text_tokenizer([text], return_tensors=\"pt\")\n",
    "    text_emb = text_encoder(**text_inputs.to(\"cuda\")).last_hidden_state\n",
    "    \n",
    "    # Fuse conditioning\n",
    "    cond_emb = conditioner(text_emb, style_emb.unsqueeze(1))\n",
    "    \n",
    "    # Sample latents\n",
    "    latents = torch.randn((1, 4, 32, 32), device=\"cuda\", dtype=torch.float16)\n",
    "    \n",
    "    # Denoise\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    for t in scheduler.timesteps:\n",
    "        noise_pred = transformer(\n",
    "            latents,\n",
    "            t,\n",
    "            encoder_hidden_states=cond_emb\n",
    "        ).sample\n",
    "        \n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    \n",
    "    # Decode\n",
    "    image = vae.decode(latents / 0.18215).sample\n",
    "    return image.detach().cpu().permute(0, 2, 3, 1).numpy()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
