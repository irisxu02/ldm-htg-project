{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c49434d-15c8-4a2c-82bf-eca4162aa5e8",
   "metadata": {},
   "source": [
    "# Implementation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d573b0c-6409-4602-b476-b99e59f6bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataset import IAMLinesDataset, IAMLinesDatasetPyTorch\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e90d79-8790-4f99-bdcd-026e801d4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Dataset parameters\n",
    "        self.img_size = (64, 256)  # Height, Width - Adjusted for line-level segments\n",
    "        self.batch_size = 64 # Reduced for potential memory issues on MPS, adjust as needed\n",
    "        self.num_workers = 0 # Often set to 0 for MPS compatibility issues\n",
    "\n",
    "        # VAE parameters\n",
    "        self.latent_dim = 512  # Latent dimension for the VAE (flattened)\n",
    "        self.vae_latent_channels = 256 # Channel dimension in VAE bottleneck (spatial)\n",
    "        self.vae_lr = 1e-4\n",
    "        self.vae_epochs = 3 # Keep low for testing\n",
    "\n",
    "        # Diffusion model parameters\n",
    "        self.timesteps = 1000  # Number of diffusion steps\n",
    "        self.beta_start = 1e-4  # Starting noise schedule value\n",
    "        self.beta_end = 2e-2  # Ending noise schedule value\n",
    "        self.diffusion_lr = 1e-4\n",
    "        self.diffusion_epochs = 5 # Keep low for testing\n",
    "\n",
    "        # Style and content encoder parameters\n",
    "        self.style_dim = 256\n",
    "        self.content_dim = 256\n",
    "        self.vocab_size = 128 # Example vocab size for ContentEncoder\n",
    "        self.max_seq_len = 100 # Example max sequence length\n",
    "\n",
    "        # Training parameters\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "            print(\"Using MPS device.\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            print(\"Using CUDA device.\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Using CPU device.\")\n",
    "        self.save_dir = \"./models\"\n",
    "        self.results_dir = \"./results\"\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76bdaf49-cc48-4cc7-9909-4d4eb4fe1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and preprocessing\n",
    "class IAMDatasetWrapper:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(config.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Load the IAM dataset and create train/val/test splits\"\"\"\n",
    "        print(\"Loading IAM dataset...\")\n",
    "        # Assuming IAMLinesDatasetPyTorch is already implemented\n",
    "        data_path = 'data/lines.tgz' \n",
    "        xml_path = 'data/xml.tgz'    \n",
    "        iam_dataset = IAMLinesDataset(data_path, xml_path)\n",
    "        full_dataset = IAMLinesDatasetPyTorch(iam_dataset=iam_dataset, transform=self.transform)\n",
    "        \n",
    "        # Split into train/val/test\n",
    "        train_size = int(0.8 * len(full_dataset))\n",
    "        val_size = int(0.1 * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded. Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c105ea-c9f4-4fc5-a9c3-9ff51d300b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VAE, self).__init__()\n",
    "        self.config = config\n",
    "        self.latent_dim = config.latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the encoder output\n",
    "        h, w = config.img_size\n",
    "        self.encoder_output_size = (h // 16, w // 16, 256)\n",
    "        self.encoder_flattened_dim = self.encoder_output_size[0] * self.encoder_output_size[1] * self.encoder_output_size[2]\n",
    "        \n",
    "        # Mean and log variance layers\n",
    "        self.fc_mu = nn.Linear(self.encoder_flattened_dim, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.encoder_flattened_dim, self.latent_dim)\n",
    "        \n",
    "        # Decoder input layer\n",
    "        self.decoder_input = nn.Linear(self.latent_dim, self.encoder_flattened_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output in range [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1, 256, self.encoder_output_size[0], self.encoder_output_size[1])\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Style Encoder\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(StyleEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # Projection head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, config.style_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        style = self.fc(x)\n",
    "        return style\n",
    "\n",
    "# Content Encoder (for text)\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, config, embedding_dim=128):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(config.vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layers\n",
    "        lstm_hidden_size = 256\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Projection head (adjust input size based on bidirectional LSTM)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 2, config.content_dim) # 2 for bidirectional\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        batch_size = x.shape[0]\n",
    "        embedded = self.embedding(x) # Shape: [B, seq_len, embedding_dim]\n",
    "\n",
    "        # LSTM forward pass\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(2 * 2, batch_size, self.lstm.hidden_size).to(x.device) # num_layers * num_directions\n",
    "        c0 = torch.zeros(2 * 2, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "        output, (hidden, _) = self.lstm(embedded, (h0, c0)) # hidden shape: [num_layers * num_directions, B, hidden_size]\n",
    "\n",
    "        # Concatenate final forward and backward hidden states from the last layer\n",
    "        # hidden[-2] is the last forward state, hidden[-1] is the last backward state\n",
    "        hidden_cat = torch.cat([hidden[-2, :, :], hidden[-1, :, :]], dim=1) # Shape: [B, hidden_size * 2]\n",
    "\n",
    "        # Project to content dimension\n",
    "        content = self.fc(hidden_cat) # Shape: [B, content_dim]\n",
    "        return content\n",
    "\n",
    "\n",
    "# UNet model for diffusion\n",
    "class DiffusionUNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DiffusionUNet, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Calculate latent image dimensions (output of VAE encoder bottleneck)\n",
    "        h, w = config.img_size\n",
    "        self.latent_h = h // 16 # e.g., 64 // 16 = 4\n",
    "        self.latent_w = w // 16 # e.g., 256 // 16 = 16\n",
    "\n",
    "        # VAE bottleneck channels (should match VAE encoder output channels)\n",
    "        vae_latent_channels = config.vae_latent_channels # e.g., 256\n",
    "\n",
    "        # Conditioning embedding dimension (size after projecting time, style, content)\n",
    "        cond_embed_dim = 128 # An intermediate dimension for embeddings\n",
    "\n",
    "        # --- Time and Conditioning Embeddings ---\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, cond_embed_dim * 4), # Project to a higher dim first\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(cond_embed_dim * 4, cond_embed_dim), # Project back to target dim\n",
    "        )\n",
    "        # Use MLP for style/content embedding projection for potentially better capacity\n",
    "        self.style_embed = nn.Sequential(\n",
    "             nn.Linear(config.style_dim, cond_embed_dim * 2),\n",
    "             nn.SiLU(),\n",
    "             nn.Linear(cond_embed_dim * 2, cond_embed_dim)\n",
    "        )\n",
    "        self.content_embed = nn.Sequential(\n",
    "             nn.Linear(config.content_dim, cond_embed_dim * 2),\n",
    "             nn.SiLU(),\n",
    "             nn.Linear(cond_embed_dim * 2, cond_embed_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "        # --- Initial Convolution Layer ---\n",
    "        # Input channels = VAE latent channels + conditioning embedding dimension\n",
    "        initial_in_channels = vae_latent_channels + cond_embed_dim # e.g., 256 + 128 = 384\n",
    "        initial_out_channels = 256 # Desired channels for the first down block\n",
    "\n",
    "        # Add GroupNorm and SiLU after initial conv\n",
    "        self.initial_conv = nn.Sequential(\n",
    "             nn.Conv2d(initial_in_channels, initial_out_channels, kernel_size=3, padding=1),\n",
    "             nn.GroupNorm(32, initial_out_channels), # Normalize after conv\n",
    "             nn.SiLU() # Activation\n",
    "        )\n",
    "\n",
    "\n",
    "        # --- Down blocks ---\n",
    "        # Structure assumes ResNet-like blocks where skip is taken before pooling\n",
    "        # Let's define channels and create blocks\n",
    "        down_channels = [initial_out_channels, 256, 512, 512] # Input channels for each level\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "        for i in range(len(down_channels) - 1):\n",
    "             self.down_blocks.append(self._make_down_block(down_channels[i], down_channels[i+1]))\n",
    "\n",
    "        # --- Middle block ---\n",
    "        self.mid_block = nn.Sequential(\n",
    "            nn.Conv2d(down_channels[-1], down_channels[-1], kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, down_channels[-1]),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(down_channels[-1], down_channels[-1], kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, down_channels[-1]),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # --- Up blocks ---\n",
    "        # Input channels need to account for skip connections\n",
    "        up_channels = down_channels[::-1] # [512, 512, 256, 256] (Output channels of mid/down blocks)\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        for i in range(len(up_channels) - 1):\n",
    "            # Input channels = output channels from previous up-block + skip connection channels from corresponding down-block\n",
    "            in_ch = up_channels[i] + down_channels[-1-i] # e.g., up1: 512(mid) + 512(d3_skip), up2: 512(u1) + 512(d2_skip) ...\n",
    "            out_ch = up_channels[i+1] # Target output channels for this block\n",
    "            self.up_blocks.append(self._make_up_block(in_ch, out_ch))\n",
    "\n",
    "        # --- Output projection ---\n",
    "        # Output should predict noise, matching the VAE latent channels\n",
    "        self.out_proj = nn.Conv2d(up_channels[-1], vae_latent_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def _make_down_block(self, in_channels, out_channels):\n",
    "        # Simpler block: Conv -> Norm -> Act -> Conv -> Norm -> Act -> Pool\n",
    "        # Returns the block and expects skip connection to be handled externally if needed\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2) # Pool at the end\n",
    "        )\n",
    "\n",
    "    def _make_up_block(self, in_channels, out_channels):\n",
    "        # Upsample -> Conv -> Norm -> Act -> Conv -> Norm -> Act\n",
    "        return nn.Sequential(\n",
    "             nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), # Adjust channels after upsampling & concat\n",
    "             nn.GroupNorm(32, out_channels),\n",
    "             nn.SiLU(),\n",
    "             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "             nn.GroupNorm(32, out_channels),\n",
    "             nn.SiLU(),\n",
    "         )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, style: torch.Tensor, content: torch.Tensor) -> torch.Tensor:\n",
    "        # x input shape expected: [B, vae_latent_channels, H', W'] e.g., [B, 256, 4, 16]\n",
    "        # t input shape: [B]\n",
    "        # style input shape: [B, style_dim]\n",
    "        # content input shape: [B, content_dim]\n",
    "\n",
    "        # 1. Embeddings\n",
    "        t = t.unsqueeze(-1).float() # [B, 1]\n",
    "        t_emb = self.time_embed(t) # [B, cond_embed_dim]\n",
    "        style_emb = self.style_embed(style) # [B, cond_embed_dim]\n",
    "        content_emb = self.content_embed(content) # [B, cond_embed_dim]\n",
    "\n",
    "        # Combine embeddings and expand spatially\n",
    "        # Add embeddings element-wise\n",
    "        cond_emb = t_emb + style_emb + content_emb # [B, cond_embed_dim]\n",
    "        cond_emb = cond_emb.unsqueeze(-1).unsqueeze(-1) # [B, cond_embed_dim, 1, 1]\n",
    "        cond_emb = cond_emb.expand(-1, -1, self.latent_h, self.latent_w) # [B, cond_embed_dim, H', W']\n",
    "\n",
    "        # 2. Concatenate input x with spatial conditioning\n",
    "        x_cat = torch.cat([x, cond_emb], dim=1) # [B, vae_latent_channels + cond_embed_dim, H', W'] e.g. [B, 384, 4, 16]\n",
    "\n",
    "        # 3. Initial Convolution\n",
    "        h = self.initial_conv(x_cat) # [B, initial_out_channels, H', W'] e.g. [B, 256, 4, 16]\n",
    "\n",
    "        # 4. Down Path (Encoder) - Store outputs for skip connections\n",
    "        skip_connections = [h] # Store initial conv output as first \"skip\"\n",
    "        for block in self.down_blocks:\n",
    "            h = block[:-1](h) # Apply convs/norms/activations\n",
    "            skip_connections.append(h) # Store output *before* pooling\n",
    "            h = block[-1](h) # Apply pooling\n",
    "        # Now h is the output after the last down block's pooling\n",
    "\n",
    "        # 5. Middle Path\n",
    "        h = self.mid_block(h) # Apply middle block convolutions\n",
    "\n",
    "        # 6. Up Path (Decoder) - Use skip connections\n",
    "        for i, block in enumerate(self.up_blocks):\n",
    "            skip = skip_connections.pop() # Get corresponding skip connection (last stored is first needed)\n",
    "            # Upsample h (using the first layer of the up_block)\n",
    "            h = block[0](h) # Apply Upsample layer\n",
    "\n",
    "            # Check spatial alignment before concatenation (optional but good practice)\n",
    "            if h.shape[2:] != skip.shape[2:]:\n",
    "                 # If sizes don't match (e.g., due to odd dimensions + pooling), resize h\n",
    "                 print(f\"Warning: Resizing skip connection {skip.shape} to match upsampled {h.shape}\")\n",
    "                 h = F.interpolate(h, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Concatenate upsampled h with the skip connection\n",
    "            h = torch.cat([h, skip], dim=1) # Concatenate along channel dimension\n",
    "\n",
    "            # Apply the rest of the up_block layers (convs, norms, activations)\n",
    "            h = block[1:](h)\n",
    "\n",
    "        # 7. Output Projection\n",
    "        output = self.out_proj(h) # [B, vae_latent_channels, H', W']\n",
    "\n",
    "        # Ensure output shape matches input shape (important!)\n",
    "        if output.shape != x.shape:\n",
    "            raise ValueError(f\"UNet output shape {output.shape} does not match input shape {x.shape}\")\n",
    "\n",
    "        return output # Shape: [B, vae_latent_channels, H', W'] e.g. [B, 256, 4, 16]\n",
    "\n",
    "class LatentDiffusionModel:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "\n",
    "        # Define beta schedule\n",
    "        self.beta = torch.linspace(config.beta_start, config.beta_end, config.timesteps, device=self.device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        # Initialize models\n",
    "        self.vae = VAE(config).to(self.device)\n",
    "        self.style_encoder = StyleEncoder(config).to(self.device)\n",
    "        # Pass config to ContentEncoder\n",
    "        self.content_encoder = ContentEncoder(config).to(self.device)\n",
    "        self.diffusion_model = DiffusionUNet(config).to(self.device)\n",
    "\n",
    "        # Define optimizers\n",
    "        self.vae_optimizer = optim.Adam(self.vae.parameters(), lr=config.vae_lr)\n",
    "        # Group diffusion model parameters with style/content encoders for joint training\n",
    "        diffusion_params = list(self.diffusion_model.parameters()) + \\\n",
    "                           list(self.style_encoder.parameters()) + \\\n",
    "                           list(self.content_encoder.parameters())\n",
    "        self.diffusion_optimizer = optim.Adam(diffusion_params, lr=config.diffusion_lr)\n",
    "\n",
    "    def train_vae(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = None) -> None:\n",
    "        \"\"\"Train the VAE model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.vae_epochs\n",
    "\n",
    "        print(\"Training VAE...\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.vae.train()\n",
    "            train_loss = 0.0\n",
    "            recon_loss_total = 0.0\n",
    "            kl_loss_total = 0.0\n",
    "\n",
    "            progress_bar = tqdm(train_loader, desc=f\"VAE Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "            for batch in progress_bar:\n",
    "                self.vae_optimizer.zero_grad()\n",
    "\n",
    "                images = batch['image'].to(self.device) # Shape: [B, 1, H, W]\n",
    "\n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "\n",
    "                # Compute loss\n",
    "                # Ensure reconstruction loss is calculated correctly, maybe scale invariant\n",
    "                recon_loss = F.mse_loss(recon_images, images, reduction='sum') / images.shape[0] # Per-sample MSE sum\n",
    "                # KL divergence loss (ensure it's summed over latent dims and averaged over batch)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1) # Sum over latent dim\n",
    "                kl_loss = torch.mean(kl_loss) # Average over batch\n",
    "\n",
    "                # Adjust KL weight if needed (can start small and increase)\n",
    "                kl_weight = 0.0001 # Example weight, tune this\n",
    "                loss = recon_loss + kl_weight * kl_loss\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                # Gradient clipping (optional but often helpful)\n",
    "                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)\n",
    "                self.vae_optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                recon_loss_total += recon_loss.item()\n",
    "                kl_loss_total += kl_loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item(), recon=recon_loss.item(), kl=kl_loss.item())\n",
    "\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            recon_loss_total /= len(train_loader)\n",
    "            kl_loss_total /= len(train_loader)\n",
    "\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_recon, val_kl = self._validate_vae(val_loader)\n",
    "\n",
    "            print(f\"VAE Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} (Recon: {recon_loss_total:.4f}, KL: {kl_loss_total:.4f}) | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} (Recon: {val_recon:.4f}, KL: {val_kl:.4f})\")\n",
    "\n",
    "            # Save checkpoint (example: save every epoch or best model)\n",
    "            # if (epoch + 1) % 1 == 0: # Save every epoch for debugging\n",
    "            self._save_checkpoint(f\"vae_epoch_{epoch+1}.pt\", model=self.vae)\n",
    "            # Add logic here to save the best model based on validation loss\n",
    "\n",
    "        print(\"VAE training completed.\")\n",
    "\n",
    "    def _validate_vae(self, val_loader: DataLoader) -> tuple[float, float, float]:\n",
    "        \"\"\"Validate the VAE model\"\"\"\n",
    "        self.vae.eval()\n",
    "        val_loss = 0.0\n",
    "        recon_loss_total = 0.0\n",
    "        kl_loss_total = 0.0\n",
    "        kl_weight = 0.0001 # Use same weight as training for comparison\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "\n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images, reduction='sum') / images.shape[0]\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "                kl_loss = torch.mean(kl_loss)\n",
    "                loss = recon_loss + kl_weight * kl_loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                recon_loss_total += recon_loss.item()\n",
    "                kl_loss_total += kl_loss.item()\n",
    "\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        recon_loss_total /= len(val_loader)\n",
    "        kl_loss_total /= len(val_loader)\n",
    "        return val_loss, recon_loss_total, kl_loss_total\n",
    "\n",
    "    # --- REWRITTEN train_diffusion ---\n",
    "    def train_diffusion(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Train the Diffusion UNet model along with Style and Content Encoders,\n",
    "        using the frozen VAE to operate in the latent space.\n",
    "        \"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.diffusion_epochs\n",
    "\n",
    "        print(\"Training Diffusion Model (UNet + Style/Content Encoders)...\")\n",
    "\n",
    "        # Ensure VAE is in eval mode and its parameters are frozen\n",
    "        self.vae.eval()\n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Ensure other models are trainable\n",
    "        self.diffusion_model.train()\n",
    "        self.style_encoder.train()\n",
    "        self.content_encoder.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Diffusion Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                self.diffusion_optimizer.zero_grad()\n",
    "\n",
    "                # --- Get Data ---\n",
    "                images = batch['image'].to(self.device) # Shape: [B, 1, H, W]\n",
    "                text_list = batch['transcription'] # List of strings\n",
    "\n",
    "                # --- TODO: Implement Proper Text Tokenization ---\n",
    "                # This is a placeholder. You NEED to replace this with actual tokenization\n",
    "                # based on your dataset's vocabulary and padding strategy.\n",
    "                # Example: Use a Hugging Face tokenizer or a custom vocabulary.\n",
    "                # The output should be padded sequences of token indices.\n",
    "                current_batch_size = images.shape[0]\n",
    "                text_indices = torch.randint(0, self.config.vocab_size,\n",
    "                                             (current_batch_size, self.config.max_seq_len),\n",
    "                                             dtype=torch.long, device=self.device) # Placeholder\n",
    "                # --- End Placeholder ---\n",
    "\n",
    "                # --- VAE Encoding to Spatial Latent ---\n",
    "                with torch.no_grad():\n",
    "                    mu, logvar = self.vae.encode(images)\n",
    "                    # Sample from the latent distribution (flattened)\n",
    "                    z_flat = self.vae.reparameterize(mu, logvar) # Shape: [B, latent_dim]\n",
    "                    # Project and reshape z back to spatial latent dimensions\n",
    "                    latent_spatial = self.vae.get_spatial_latent(z_flat)\n",
    "                    # Expected shape: [B, vae_latent_channels, H/16, W/16] e.g., [64, 256, 4, 16]\n",
    "\n",
    "                # --- Diffusion Forward Process ---\n",
    "                # Sample random timesteps for each image in the batch\n",
    "                t = torch.randint(0, self.config.timesteps, (current_batch_size,), device=self.device).long()\n",
    "\n",
    "                # Sample noise matching the spatial latent shape\n",
    "                noise = torch.randn_like(latent_spatial) # Shape: [B, C, H', W']\n",
    "\n",
    "                # Get alpha_cumprod for sampled timesteps t and reshape for broadcasting\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1) # Shape: [B, 1, 1, 1]\n",
    "\n",
    "                # Calculate noisy latents (forward diffusion formula)\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + \\\n",
    "                                torch.sqrt(1.0 - alpha_cumprod_t) * noise\n",
    "                # noisy_latents shape: [B, C, H', W']\n",
    "\n",
    "                # --- Conditioning ---\n",
    "                # Encode style from images (using the original image)\n",
    "                style_features = self.style_encoder(images) # Shape: [B, style_dim]\n",
    "                # Encode content from text indices\n",
    "                content_features = self.content_encoder(text_indices) # Shape: [B, content_dim]\n",
    "\n",
    "                # --- Diffusion UNet Prediction ---\n",
    "                # Predict the noise added at timestep t, conditioned on style and content\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                # noise_pred expected shape: [B, C, H', W'] (same as noise)\n",
    "\n",
    "                # --- Loss Calculation ---\n",
    "                # Calculate loss between the predicted noise and the actual noise added\n",
    "                loss = F.mse_loss(noise_pred, noise) # Simple MSE loss is common\n",
    "\n",
    "                # --- Backward Pass & Optimization ---\n",
    "                loss.backward()\n",
    "                # Optional: Gradient clipping for diffusion model parameters\n",
    "                torch.nn.utils.clip_grad_norm_(self.diffusion_optimizer.param_groups[0]['params'], max_norm=1.0)\n",
    "                self.diffusion_optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            # --- Validation ---\n",
    "            val_loss = self._validate_diffusion(val_loader)\n",
    "\n",
    "            print(f\"Diffusion Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # --- Save Checkpoint ---\n",
    "            # Example: Save every few epochs or based on validation loss\n",
    "            if (epoch + 1) % 5 == 0: # Save every 5 epochs\n",
    "                self._save_checkpoint(\n",
    "                    f\"diffusion_epoch_{epoch+1}.pt\",\n",
    "                    models_dict={ # Save as a dictionary for clarity\n",
    "                         'diffusion_model': self.diffusion_model,\n",
    "                         'style_encoder': self.style_encoder,\n",
    "                         'content_encoder': self.content_encoder\n",
    "                     }\n",
    "                )\n",
    "            # Add logic here to save the best model based on validation loss\n",
    "\n",
    "\n",
    "        print(\"Diffusion model training completed.\")\n",
    "\n",
    "    # --- REWRITTEN _validate_diffusion ---\n",
    "    def _validate_diffusion(self, val_loader: DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        Validate the diffusion model (UNet + Style/Content Encoders) on the validation set.\n",
    "        Mirrors the training loop logic for calculating the diffusion loss.\n",
    "        \"\"\"\n",
    "        self.diffusion_model.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        # VAE should already be in eval mode from the start of train_diffusion\n",
    "\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # --- Get Data ---\n",
    "                images = batch['image'].to(self.device)\n",
    "                text_list = batch['transcription']\n",
    "                current_batch_size = images.shape[0]\n",
    "\n",
    "                # --- TODO: Implement Proper Text Tokenization (Placeholder) ---\n",
    "                text_indices = torch.randint(0, self.config.vocab_size,\n",
    "                                             (current_batch_size, self.config.max_seq_len),\n",
    "                                             dtype=torch.long, device=self.device) # Placeholder\n",
    "                # --- End Placeholder ---\n",
    "\n",
    "                # --- VAE Encoding to Spatial Latent ---\n",
    "                # Use VAE mean for validation/inference, or reparameterize like in training\n",
    "                # Using reparameterization mirrors training loss calculation better\n",
    "                mu, logvar = self.vae.encode(images)\n",
    "                z_flat = self.vae.reparameterize(mu, logvar) # Or just use mu for deterministic validation\n",
    "                latent_spatial = self.vae.get_spatial_latent(z_flat) # Shape: [B, C, H', W']\n",
    "\n",
    "                # --- Diffusion Forward Process ---\n",
    "                t = torch.randint(0, self.config.timesteps, (current_batch_size,), device=self.device).long()\n",
    "                noise = torch.randn_like(latent_spatial)\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + \\\n",
    "                                torch.sqrt(1.0 - alpha_cumprod_t) * noise\n",
    "\n",
    "                # --- Conditioning ---\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "\n",
    "                # --- Diffusion UNet Prediction ---\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "\n",
    "                # --- Loss Calculation ---\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Switch models back to train mode after validation if needed within the epoch loop\n",
    "        # (Done automatically at the start of the next training epoch loop iteration)\n",
    "        self.diffusion_model.train()\n",
    "        self.style_encoder.train()\n",
    "        self.content_encoder.train()\n",
    "\n",
    "        return val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "    def _save_checkpoint(self, filename: str, model: nn.Module = None, models_dict: dict = None) -> None:\n",
    "        \"\"\"Save model checkpoint(s)\"\"\"\n",
    "        save_path = os.path.join(self.config.save_dir, filename)\n",
    "\n",
    "        if model is not None:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved single model checkpoint to {save_path}\")\n",
    "        elif models_dict is not None:\n",
    "            save_state = {name: m.state_dict() for name, m in models_dict.items()}\n",
    "            torch.save(save_state, save_path)\n",
    "            print(f\"Saved multiple model checkpoints to {save_path}\")\n",
    "        else:\n",
    "             print(\"Warning: _save_checkpoint called without model or models_dict.\")\n",
    "\n",
    "\n",
    "    # --- Generation function (kept similar, check spatial latent generation) ---\n",
    "    @torch.no_grad() # Decorator for no_grad context\n",
    "    def generate_handwriting(self, style_image: torch.Tensor, text: list[str], guidance_scale: float = 7.5) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate handwriting using the trained models with DDPM sampling and optional classifier-free guidance.\n",
    "\n",
    "        Args:\n",
    "            style_image: A single preprocessed style image tensor [1, H, W].\n",
    "            text: A list containing the text string to generate.\n",
    "            guidance_scale: Scale for classifier-free guidance. 0 means no guidance.\n",
    "\n",
    "        Returns:\n",
    "            A tensor representing the generated handwriting image [1, 1, H, W].\n",
    "        \"\"\"\n",
    "        # Ensure models are in eval mode\n",
    "        self.vae.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        self.diffusion_model.eval()\n",
    "\n",
    "        # --- Prepare Inputs ---\n",
    "        # Style Image\n",
    "        style_image = style_image.unsqueeze(0).to(self.device) # Add batch dim: [1, 1, H, W]\n",
    "        style_features = self.style_encoder(style_image) # [1, style_dim]\n",
    "\n",
    "        # Text\n",
    "        # TODO: Use the same tokenization as in training for the input text\n",
    "        text_indices = torch.randint(0, self.config.vocab_size,\n",
    "                                      (1, self.config.max_seq_len),\n",
    "                                      dtype=torch.long, device=self.device) # Placeholder [1, seq_len]\n",
    "        content_features = self.content_encoder(text_indices) # [1, content_dim]\n",
    "\n",
    "        # --- Prepare Unconditional Inputs (for Classifier-Free Guidance) ---\n",
    "        # Often uses zeroed or averaged embeddings, or requires training with dropped conditioning\n",
    "        # Placeholder: Use zero embeddings for unconditional generation\n",
    "        uncond_style_features = torch.zeros_like(style_features).to(self.device)\n",
    "        uncond_content_features = torch.zeros_like(content_features).to(self.device)\n",
    "        # If trained with conditioning dropout, use specific null embeddings\n",
    "\n",
    "        # Combine conditional and unconditional features if using guidance\n",
    "        if guidance_scale > 0:\n",
    "            style_features = torch.cat([uncond_style_features, style_features], dim=0) # [2, style_dim]\n",
    "            content_features = torch.cat([uncond_content_features, content_features], dim=0) # [2, content_dim]\n",
    "            batch_size = 2\n",
    "        else:\n",
    "            batch_size = 1 # Only conditional generation\n",
    "\n",
    "        # --- Sampling Loop (DDPM) ---\n",
    "        # Start with random noise in the VAE latent space\n",
    "        latent_h = self.config.img_size[0] // 16\n",
    "        latent_w = self.config.img_size[1] // 16\n",
    "        latent_channels = self.config.vae_latent_channels\n",
    "        latent_shape = (batch_size, latent_channels, latent_h, latent_w)\n",
    "        latents = torch.randn(latent_shape, device=self.device) # Start with noise z_T\n",
    "\n",
    "        progress_bar = tqdm(range(self.config.timesteps - 1, -1, -1), desc=\"Generating\", leave=False)\n",
    "        for i in progress_bar:\n",
    "            t = torch.full((batch_size,), i, device=self.device, dtype=torch.long) # Timestep tensor [batch_size]\n",
    "\n",
    "            # Predict noise (handles both conditional and unconditional if batch_size=2)\n",
    "            noise_pred = self.diffusion_model(latents, t, style_features, content_features)\n",
    "\n",
    "            # Classifier-Free Guidance\n",
    "            if guidance_scale > 0:\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                # Combine predictions: noise = uncond + scale * (cond - uncond)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "                # Now noise_pred has shape [1, C, H', W'] if guidance was applied\n",
    "\n",
    "            # --- DDPM Sampling Step ---\n",
    "            # Get coefficients for timestep t\n",
    "            alpha_t = self.alpha[i]\n",
    "            alpha_cumprod_t = self.alpha_cumprod[i]\n",
    "            beta_t = self.beta[i]\n",
    "\n",
    "            # Calculate mean prediction (denoised latent estimate)\n",
    "            # Based on Ho et al. (2020) DDPM paper, Eq. 11 & 15 implementation details\n",
    "            coeff1 = 1.0 / torch.sqrt(alpha_t)\n",
    "            coeff2 = beta_t / torch.sqrt(1.0 - alpha_cumprod_t)\n",
    "            mean_pred = coeff1 * (latents - coeff2 * noise_pred)\n",
    "\n",
    "            # Sample noise for the step (unless it's the last step)\n",
    "            if i > 0:\n",
    "                noise = torch.randn_like(latents)\n",
    "                # Calculate variance (fixed small or learned, using beta_t for simplicity)\n",
    "                variance = torch.sqrt(beta_t) * noise\n",
    "                latents = mean_pred + variance # Update latents z_{t-1}\n",
    "            else:\n",
    "                latents = mean_pred # Final denoised latent z_0 (no noise added)\n",
    "\n",
    "            # If guidance was used, latents might have batch size 2, keep only the guided one\n",
    "            if guidance_scale > 0 and latents.shape[0] == 2:\n",
    "                 latents = latents[1:] # Keep the conditional prediction\n",
    "\n",
    "        # --- Decode Final Latent ---\n",
    "        # latents should now be the estimated clean latent [1, C, H', W']\n",
    "        # Decode using the VAE decoder\n",
    "        generated_image = self.vae.decode(latents) # Pass spatial latent directly\n",
    "\n",
    "        # Post-process: Clamp or rescale output if needed (VAE uses Tanh -> [-1, 1])\n",
    "        generated_image = (generated_image + 1.0) / 2.0 # Rescale to [0, 1]\n",
    "        generated_image = generated_image.clamp(0, 1)\n",
    "\n",
    "        return generated_image # Shape: [1, 1, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eea9d1c-92ad-4bbd-a349-5c07e8fc989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device.\n"
     ]
    }
   ],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06baa2f5-1b48-4593-af90-338207dd6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrapper = IAMDatasetWrapper(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d74a560c-56ae-469f-8e19-614a1b1227df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IAM dataset...\n",
      "Dataset loaded. Train: 10682, Val: 1335, Test: 1336\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = data_wrapper.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "616186c4-312b-480e-9f90-329e92b47fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDiffusionModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753a9e77-c738-47e0-8017-8f8c0c7672bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Epoch 1/3 | Train Loss: 1858.4385 (Recon: 1857.6365, KL: 8019.0288) | Val Loss: 1130.3636 (Recon: 1130.1324, KL: 2312.1105)\n",
      "Saved single model checkpoint to ./models/vae_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Epoch 2/3 | Train Loss: 1119.0259 (Recon: 1118.8272, KL: 1987.3379) | Val Loss: 1101.7568 (Recon: 1101.5337, KL: 2230.6409)\n",
      "Saved single model checkpoint to ./models/vae_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Epoch 3/3 | Train Loss: 1063.5880 (Recon: 1063.3850, KL: 2029.4460) | Val Loss: 1027.3713 (Recon: 1027.1740, KL: 1972.7895)\n",
      "Saved single model checkpoint to ./models/vae_epoch_3.pt\n",
      "VAE training completed.\n"
     ]
    }
   ],
   "source": [
    "model.train_vae(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed28b430-e407-4f7f-bd28-fba1d1e8df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Diffusion Model (UNet + Style/Content Encoders)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'get_spatial_latent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#mark-b\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 484\u001b[0m, in \u001b[0;36mLatentDiffusionModel.train_diffusion\u001b[0;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     z_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar) \u001b[38;5;66;03m# Shape: [B, latent_dim]\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;66;03m# Project and reshape z back to spatial latent dimensions\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     latent_spatial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spatial_latent\u001b[49m(z_flat)\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# Expected shape: [B, vae_latent_channels, H/16, W/16] e.g., [64, 256, 4, 16]\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# --- Diffusion Forward Process ---\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Sample random timesteps for each image in the batch\u001b[39;00m\n\u001b[1;32m    489\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtimesteps, (current_batch_size,), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'get_spatial_latent'"
     ]
    }
   ],
   "source": [
    "model.train_diffusion(train_loader, val_loader) #mark-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec56b2ea-aa04-479f-99d4-b11eb849b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # Then train diffusion model\n",
    "    model.train_diffusion(train_loader, val_loader)\n",
    "    \n",
    "    # Generate some samples\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= 5:  # Generate 5 samples\n",
    "            break\n",
    "        \n",
    "        # Get a sample for style reference\n",
    "        style_image = batch['image'][0]\n",
    "        text = batch['transcription'][0]\n",
    "        \n",
    "        # Generate new handwriting with the same style but different text\n",
    "        generated = model.generate_handwriting(style_image, \"This is a generated sample.\")\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Original\")\n",
    "        plt.imshow(style_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Generated\")\n",
    "        plt.imshow(generated.squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.savefig(os.path.join(config.results_dir, f\"sample_{i}.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22552b-5c70-4a92-9794-d1a40a8ca079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
