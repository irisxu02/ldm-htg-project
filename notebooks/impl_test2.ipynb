{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c49434d-15c8-4a2c-82bf-eca4162aa5e8",
   "metadata": {},
   "source": [
    "# Implementation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d573b0c-6409-4602-b476-b99e59f6bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataset import IAMLinesDataset, IAMLinesDatasetPyTorch\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52e90d79-8790-4f99-bdcd-026e801d4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Dataset parameters\n",
    "        self.img_size = (64, 256)  # Height, Width - Adjusted for line-level segments\n",
    "        self.batch_size = 64\n",
    "        self.num_workers = 4\n",
    "        \n",
    "        # VAE parameters\n",
    "        self.latent_dim = 512  # Latent dimension for the VAE\n",
    "        self.vae_lr = 1e-4\n",
    "        self.vae_epochs = 2 #orig: 50\n",
    "        \n",
    "        # Diffusion model parameters\n",
    "        self.timesteps = 1000  # Number of diffusion steps\n",
    "        self.beta_start = 1e-4  # Starting noise schedule value\n",
    "        self.beta_end = 2e-2  # Ending noise schedule value\n",
    "        self.diffusion_lr = 1e-4\n",
    "        self.diffusion_epochs = 5 #orig: 100\n",
    "        \n",
    "        # Style and content encoder parameters\n",
    "        self.style_dim = 256\n",
    "        self.content_dim = 256\n",
    "        \n",
    "        # Training parameters\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "            print(\"Using mac gpu\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            print(\"Using cuda gpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Using cpu\")\n",
    "        self.save_dir = \"./models\"\n",
    "        self.results_dir = \"./results\"\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76bdaf49-cc48-4cc7-9909-4d4eb4fe1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and preprocessing\n",
    "class IAMDatasetWrapper:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(config.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Load the IAM dataset and create train/val/test splits\"\"\"\n",
    "        print(\"Loading IAM dataset...\")\n",
    "        # Assuming IAMLinesDatasetPyTorch is already implemented\n",
    "        data_path = 'data/lines.tgz' \n",
    "        xml_path = 'data/xml.tgz'    \n",
    "        iam_dataset = IAMLinesDataset(data_path, xml_path)\n",
    "        full_dataset = IAMLinesDatasetPyTorch(iam_dataset=iam_dataset, transform=self.transform)\n",
    "        \n",
    "        # Split into train/val/test\n",
    "        train_size = int(0.8 * len(full_dataset))\n",
    "        val_size = int(0.1 * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded. Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94c105ea-c9f4-4fc5-a9c3-9ff51d300b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VAE, self).__init__()\n",
    "        self.config = config\n",
    "        self.latent_dim = config.latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the encoder output\n",
    "        h, w = config.img_size\n",
    "        self.encoder_output_size = (h // 16, w // 16, 256)\n",
    "        self.encoder_flattened_dim = self.encoder_output_size[0] * self.encoder_output_size[1] * self.encoder_output_size[2]\n",
    "        \n",
    "        # Mean and log variance layers\n",
    "        self.fc_mu = nn.Linear(self.encoder_flattened_dim, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.encoder_flattened_dim, self.latent_dim)\n",
    "        \n",
    "        # Decoder input layer\n",
    "        self.decoder_input = nn.Linear(self.latent_dim, self.encoder_flattened_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output in range [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1, 256, self.encoder_output_size[0], self.encoder_output_size[1])\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Style Encoder\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(StyleEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # Projection head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, config.style_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        style = self.fc(x)\n",
    "        return style\n",
    "\n",
    "# Content Encoder (for text)\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, config, vocab_size=128, embedding_dim=128):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Projection head\n",
    "        self.fc = nn.Linear(512, config.content_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Concatenate forward and backward hidden states\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Project to content dimension\n",
    "        content = self.fc(hidden)\n",
    "        return content\n",
    "\n",
    "# Diffusion Model\n",
    "class LatentDiffusionModel:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Define beta schedule\n",
    "        self.beta = torch.linspace(config.beta_start, config.beta_end, config.timesteps).to(self.device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.vae = VAE(config).to(self.device)\n",
    "        self.style_encoder = StyleEncoder(config).to(self.device)\n",
    "        self.content_encoder = ContentEncoder(config).to(self.device)\n",
    "        self.diffusion_model = DiffusionUNet(config).to(self.device)\n",
    "        \n",
    "        # Define optimizers\n",
    "        self.vae_optimizer = optim.Adam(self.vae.parameters(), lr=config.vae_lr)\n",
    "        self.style_optimizer = optim.Adam(self.style_encoder.parameters(), lr=config.diffusion_lr)\n",
    "        self.content_optimizer = optim.Adam(self.content_encoder.parameters(), lr=config.diffusion_lr)\n",
    "        self.diffusion_optimizer = optim.Adam(self.diffusion_model.parameters(), lr=config.diffusion_lr)\n",
    "    \n",
    "    def train_vae(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Train the VAE model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.vae_epochs\n",
    "        \n",
    "        print(\"Training VAE...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.vae.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                self.vae_optimizer.zero_grad()\n",
    "                \n",
    "                # Get handwriting images\n",
    "                images = batch['image'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "                \n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = recon_loss + 0.001 * kl_loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.vae_optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self._validate_vae(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self._save_checkpoint(f\"vae_epoch_{epoch+1}.pt\", model=self.vae)\n",
    "        \n",
    "        print(\"VAE training completed.\")\n",
    "    \n",
    "    def _validate_vae(self, val_loader):\n",
    "        \"\"\"Validate the VAE model\"\"\"\n",
    "        self.vae.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "                \n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = recon_loss + 0.001 * kl_loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        return val_loss / len(val_loader)\n",
    "    \n",
    "    def train_diffusion(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Train the diffusion model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.diffusion_epochs\n",
    "        \n",
    "        print(\"Training Diffusion Model...\")\n",
    "        \n",
    "        # Ensure VAE is in eval mode and frozen\n",
    "        self.vae.eval()\n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Calculate latent dimensions for clarity\n",
    "        latent_h = self.config.img_size[0] // 16  # Height divided by total downsampling factor\n",
    "        latent_w = self.config.img_size[1] // 16  # Width divided by total downsampling factor\n",
    "        latent_channels = 256  # Number of channels in the VAE bottleneck\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.diffusion_model.train()\n",
    "            self.style_encoder.train()\n",
    "            self.content_encoder.train()\n",
    "            \n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                # Zero gradients\n",
    "                self.diffusion_optimizer.zero_grad()\n",
    "                self.style_optimizer.zero_grad()\n",
    "                self.content_optimizer.zero_grad()\n",
    "                \n",
    "                # Get data\n",
    "                images = batch['image'].to(self.device)\n",
    "                text = batch['transcription']\n",
    "                \n",
    "                # Process text data - create proper tokenization\n",
    "                # For simplicity, using a fixed length and padding\n",
    "                max_seq_len = 100\n",
    "                text_indices = torch.zeros(len(text), max_seq_len).long().to(self.device)\n",
    "                for i, t in enumerate(text):\n",
    "                    # Convert string to character indices (simplified)\n",
    "                    # In production, use a proper tokenizer\n",
    "                    for j, char in enumerate(t[:max_seq_len]):\n",
    "                        text_indices[i, j] = ord(char) % 128  # Simple char to index mapping\n",
    "                \n",
    "                # Extract spatial latent representations from the VAE\n",
    "                with torch.no_grad():\n",
    "                    # Get encoder outputs\n",
    "                    encoder_output = self.vae.encoder(images)\n",
    "                    # Flatten for mu/logvar calculation\n",
    "                    encoder_flat = encoder_output.view(images.shape[0], -1)\n",
    "                    # Get latent distribution parameters\n",
    "                    mu = self.vae.fc_mu(encoder_flat)\n",
    "                    logvar = self.vae.fc_logvar(encoder_flat)\n",
    "                    # Sample from distribution\n",
    "                    z_flat = self.vae.reparameterize(mu, logvar)\n",
    "                    \n",
    "                    # Map back to spatial representation\n",
    "                    z_projected = self.vae.decoder_input(z_flat)\n",
    "                    # Reshape to spatial dimensions expected by diffusion model\n",
    "                    # [batch_size, channels, height, width]\n",
    "                    latent_spatial = z_projected.view(\n",
    "                        images.shape[0],\n",
    "                        latent_channels,\n",
    "                        latent_h,\n",
    "                        latent_w\n",
    "                    )\n",
    "                \n",
    "                # Sample random timesteps for each image\n",
    "                t = torch.randint(0, self.config.timesteps, (images.shape[0],), device=self.device)\n",
    "                \n",
    "                # Add noise to latent images\n",
    "                noise = torch.randn_like(latent_spatial)\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "                \n",
    "                # Calculate noisy latents\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                \n",
    "                # Encode style and content\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "                # Predict noise using the diffusion UNet\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "                # Compute loss between predicted and actual noise\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Apply gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.diffusion_model.parameters(), max_norm=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.style_encoder.parameters(), max_norm=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.content_encoder.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.diffusion_optimizer.step()\n",
    "                self.style_optimizer.step()\n",
    "                self.content_optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Calculate average training loss\n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation after each epoch\n",
    "            val_loss = self._validate_diffusion(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:  # Save more frequently\n",
    "                self._save_checkpoint(\n",
    "                    f\"diffusion_epoch_{epoch+1}.pt\", \n",
    "                    models=[self.diffusion_model, self.style_encoder, self.content_encoder]\n",
    "                )\n",
    "        \n",
    "        print(\"Diffusion model training completed!\")\n",
    "    \n",
    "    def _validate_diffusion(self, val_loader):\n",
    "        \"\"\"Validate the diffusion model\"\"\"\n",
    "        # Set models to evaluation mode\n",
    "        self.diffusion_model.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Calculate latent dimensions\n",
    "        latent_h = self.config.img_size[0] // 16\n",
    "        latent_w = self.config.img_size[1] // 16\n",
    "        latent_channels = 256\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Get data\n",
    "                images = batch['image'].to(self.device)\n",
    "                text = batch['transcription']\n",
    "                \n",
    "                # Process text data\n",
    "                max_seq_len = 100\n",
    "                text_indices = torch.zeros(len(text), max_seq_len).long().to(self.device)\n",
    "                for i, t in enumerate(text):\n",
    "                    for j, char in enumerate(t[:max_seq_len]):\n",
    "                        text_indices[i, j] = ord(char) % 128\n",
    "                \n",
    "                # Extract spatial latents from VAE\n",
    "                encoder_output = self.vae.encoder(images)\n",
    "                encoder_flat = encoder_output.view(images.shape[0], -1)\n",
    "                mu = self.vae.fc_mu(encoder_flat)\n",
    "                logvar = self.vae.fc_logvar(encoder_flat)\n",
    "                z_flat = self.vae.reparameterize(mu, logvar)\n",
    "                \n",
    "                # Map to spatial representation\n",
    "                z_projected = self.vae.decoder_input(z_flat)\n",
    "                latent_spatial = z_projected.view(\n",
    "                    images.shape[0], \n",
    "                    latent_channels,\n",
    "                    latent_h,\n",
    "                    latent_w\n",
    "                )\n",
    "                \n",
    "                # Sample random timesteps\n",
    "                t = torch.randint(0, self.config.timesteps, (images.shape[0],), device=self.device)\n",
    "                \n",
    "                # Add noise to latents\n",
    "                noise = torch.randn_like(latent_spatial)\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                \n",
    "                # Get style and content features\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        return val_loss / num_batches\n",
    "    \n",
    "    def _save_checkpoint(self, filename, model=None, models=None):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        save_path = os.path.join(self.config.save_dir, filename)\n",
    "        \n",
    "        if model is not None:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        elif models is not None:\n",
    "            save_dict = {}\n",
    "            for i, m in enumerate(models):\n",
    "                save_dict[f\"model_{i}\"] = m.state_dict()\n",
    "            torch.save(save_dict, save_path)\n",
    "    \n",
    "    # def generate_handwriting(self, style_image, text, steps=50):\n",
    "    #     \"\"\"Generate handwriting with a given style and text\"\"\"\n",
    "    #     # Ensure models are in eval mode\n",
    "    #     self.vae.eval()\n",
    "    #     self.style_encoder.eval()\n",
    "    #     self.content_encoder.eval()\n",
    "    #     self.diffusion_model.eval()\n",
    "        \n",
    "    #     with torch.no_grad():\n",
    "    #         # Preprocess style image\n",
    "    #         style_image = style_image.unsqueeze(0).to(self.device)\n",
    "            \n",
    "    #         # Encode style\n",
    "    #         style_features = self.style_encoder(style_image)\n",
    "            \n",
    "    #         # Encode text (simplified)\n",
    "    #         text_indices = torch.zeros(1, 100).long().to(self.device)  # Placeholder\n",
    "    #         content_features = self.content_encoder(text_indices)\n",
    "            \n",
    "    #         # Start with random noise in the latent space\n",
    "    #         latent_shape = (1, 256, self.config.img_size[0] // 16, self.config.img_size[1] // 16)\n",
    "    #         latent = torch.randn(latent_shape).to(self.device)\n",
    "            \n",
    "    #         # Denoise gradually\n",
    "    #         for t in tqdm(range(self.config.timesteps - 1, -1, -1), desc=\"Generating\"):\n",
    "    #             # Get the timestep\n",
    "    #             timestep = torch.tensor([t], device=self.device)\n",
    "                \n",
    "    #             # Predict noise\n",
    "    #             noise_pred = self.diffusion_model(latent, timestep, style_features, content_features)\n",
    "                \n",
    "    #             # Get alpha values for current timestep\n",
    "    #             alpha = self.alpha[t]\n",
    "    #             alpha_cumprod = self.alpha_cumprod[t]\n",
    "    #             beta = self.beta[t]\n",
    "                \n",
    "    #             # No noise at timestep 0\n",
    "    #             if t > 0:\n",
    "    #                 noise = torch.randn_like(latent)\n",
    "    #             else:\n",
    "    #                 noise = torch.zeros_like(latent)\n",
    "                \n",
    "    #             # Update latent\n",
    "    #             latent = (1 / torch.sqrt(alpha)) * (\n",
    "    #                 latent - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * noise_pred\n",
    "    #             ) + torch.sqrt(beta) * noise\n",
    "            \n",
    "    #         # Decode latent to image\n",
    "    #         generated_image = self.vae.decode(latent)\n",
    "            \n",
    "    #         return generated_image\n",
    "    def generate_handwriting(self, style_image, text, steps=None):\n",
    "        \"\"\"Generate handwriting with a given style and text\"\"\"\n",
    "        if steps is None:\n",
    "            steps = self.config.timesteps\n",
    "        \n",
    "        # Ensure models are in eval mode\n",
    "        self.vae.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        self.diffusion_model.eval()\n",
    "        \n",
    "        # Calculate latent dimensions\n",
    "        latent_h = self.config.img_size[0] // 16\n",
    "        latent_w = self.config.img_size[1] // 16\n",
    "        latent_channels = 256\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Preprocess style image\n",
    "            if not isinstance(style_image, torch.Tensor):\n",
    "                # Convert to tensor if needed (e.g., if it's a PIL image)\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize(self.config.img_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,))\n",
    "                ])\n",
    "                style_image = transform(style_image).unsqueeze(0)\n",
    "            \n",
    "            style_image = style_image.to(self.device)\n",
    "            \n",
    "            # Encode style\n",
    "            style_features = self.style_encoder(style_image)\n",
    "            \n",
    "            # Process text input\n",
    "            max_seq_len = 100\n",
    "            text_indices = torch.zeros(1, max_seq_len).long().to(self.device)\n",
    "            for i, char in enumerate(text[:max_seq_len]):\n",
    "                text_indices[0, i] = ord(char) % 128\n",
    "            \n",
    "            # Get content features\n",
    "            content_features = self.content_encoder(text_indices)\n",
    "            \n",
    "            # Start with random noise in the latent space (spatial format)\n",
    "            latent = torch.randn(1, latent_channels, latent_h, latent_w).to(self.device)\n",
    "            \n",
    "            # Gradually denoise through reverse diffusion process\n",
    "            for i in tqdm(range(steps), desc=\"Generating handwriting\"):\n",
    "                # Get the timestep (counting backwards)\n",
    "                t = steps - i - 1\n",
    "                timestep = torch.tensor([t], device=self.device)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = self.diffusion_model(latent, timestep, style_features, content_features)\n",
    "                \n",
    "                # Get alpha values for current timestep\n",
    "                alpha = self.alpha[t]\n",
    "                alpha_cumprod = self.alpha_cumprod[t]\n",
    "                beta = self.beta[t]\n",
    "                \n",
    "                # No noise at timestep 0\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(latent)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(latent)\n",
    "                \n",
    "                # Update latent using the DDPM formula\n",
    "                latent = (1 / torch.sqrt(alpha)) * (\n",
    "                    latent - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * noise_pred\n",
    "                ) + torch.sqrt(beta) * noise\n",
    "            \n",
    "            # Decode spatial latent to image\n",
    "            # First convert spatial latent to flattened format for the VAE decoder\n",
    "            latent_flat = latent.view(1, -1)[:, :self.config.latent_dim]  # Ensure proper dimension\n",
    "            generated_image = self.vae.decode(latent_flat)\n",
    "            \n",
    "            return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77a9f882-b10b-4c22-81e3-9b6d3a18c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionUNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DiffusionUNet, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Calculate latent image dimensions\n",
    "        h, w = config.img_size\n",
    "        self.latent_h, self.latent_w = h // 16, w // 16  # 4×16 for 64×256 input\n",
    "        \n",
    "        # VAE bottleneck channels\n",
    "        vae_latent_channels = 256  # Matches VAE's spatial channels\n",
    "        \n",
    "        # Conditioning embedding dimension\n",
    "        cond_channels = 128\n",
    "        \n",
    "        # Time and conditioning embeddings\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "        self.style_embed = nn.Linear(config.style_dim, 128)\n",
    "        self.content_embed = nn.Linear(config.content_dim, 128)\n",
    "        \n",
    "        # Initial convolution - combines latent and conditioning\n",
    "        initial_in_channels = vae_latent_channels + cond_channels\n",
    "        initial_out_channels = 256\n",
    "        self.initial_conv = nn.Conv2d(initial_in_channels, initial_out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Adjusted down blocks - only use 2 instead of 3 due to small spatial dimensions\n",
    "        self.down1 = self._make_down_block(initial_out_channels, 256)  # 4×16 -> 2×8\n",
    "        self.down2 = self._make_down_block(256, 512)                  # 2×8 -> 1×4\n",
    "        \n",
    "        # Middle block\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        \n",
    "        # Adjusted up blocks\n",
    "        self.up1 = self._make_up_block(512, 256)                      # 1×4 -> 2×8 \n",
    "        self.up2 = self._make_up_block(256 + 256, 256)                # 2×8 -> 4×16\n",
    "        \n",
    "        # Output projection\n",
    "        self.out = nn.Conv2d(256, vae_latent_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def _make_down_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "    \n",
    "    def _make_up_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t, style, content):\n",
    "        # Input: [B, 256, 4, 16]\n",
    "        t = t.unsqueeze(-1).float()  # [B, 1]\n",
    "        \n",
    "        # Embeddings\n",
    "        t_emb = self.time_embed(t)  # [B, 128]\n",
    "        style_emb = self.style_embed(style)  # [B, 128]\n",
    "        content_emb = self.content_embed(content)  # [B, 128]\n",
    "        \n",
    "        # Combine and expand to spatial dimensions\n",
    "        cond_emb = (t_emb + style_emb + content_emb).unsqueeze(-1).unsqueeze(-1)  # [B, 128, 1, 1]\n",
    "        cond_emb = cond_emb.expand(-1, -1, self.latent_h, self.latent_w)  # [B, 128, 4, 16]\n",
    "        \n",
    "        # Concatenate with input\n",
    "        x_cat = torch.cat([x, cond_emb], dim=1)  # [B, 384, 4, 16]\n",
    "        \n",
    "        # Initial convolution\n",
    "        h = self.initial_conv(x_cat)  # [B, 256, 4, 16]\n",
    "        \n",
    "        # Encoder path - only 2 down blocks\n",
    "        d1 = self.down1(h)  # [B, 256, 2, 8]\n",
    "        d2 = self.down2(d1)  # [B, 512, 1, 4]\n",
    "        \n",
    "        # Middle\n",
    "        mid = self.mid(d2)  # [B, 512, 1, 4]\n",
    "        \n",
    "        # Decoder path - adjusted for 2 down blocks\n",
    "        u1 = self.up1(mid)  # [B, 256, 2, 8]\n",
    "        \n",
    "        # Skip connection with d1\n",
    "        u2_input = torch.cat([u1, d1], dim=1)  # [B, 512, 2, 8]\n",
    "        u2 = self.up2(u2_input)  # [B, 256, 4, 16]\n",
    "        \n",
    "        # Output - predict noise\n",
    "        return self.out(u2)  # [B, 256, 4, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c51a1b25-a127-4e30-8692-690acacdafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StyleEncoder(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super(StyleEncoder, self).__init__()\n",
    "#         self.config = config\n",
    "        \n",
    "#         # Calculate appropriate pooling and feature map sizes based on input dimensions\n",
    "#         h, w = config.img_size  # (64, 256)\n",
    "        \n",
    "#         # CNN backbone with careful attention to spatial dimensions\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             # Layer 1: 64×256 -> 32×128\n",
    "#             nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "            \n",
    "#             # Layer 2: 32×128 -> 16×64\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "            \n",
    "#             # Layer 3: 16×64 -> 8×32\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "            \n",
    "#             # Layer 4: 8×32 -> 4×16\n",
    "#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "#         )\n",
    "        \n",
    "#         # Calculate feature dimensions after CNN\n",
    "#         self.feature_dim = 256 * (h // 16) * (w // 16)  # 256 * 4 * 16 = 16384\n",
    "        \n",
    "#         # Projection head with dropout for regularization\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(self.feature_dim, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(512, config.style_dim),\n",
    "#             nn.LayerNorm(config.style_dim)  # Normalize final embeddings\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass through the StyleEncoder.\n",
    "        \n",
    "#         Args:\n",
    "#             x: Input handwriting images of shape [batch_size, 1, H, W]\n",
    "            \n",
    "#         Returns:\n",
    "#             style_embedding: Style embeddings of shape [batch_size, style_dim]\n",
    "#         \"\"\"\n",
    "#         features = self.cnn(x)\n",
    "#         style_embedding = self.fc(features)\n",
    "#         return style_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1716214d-5552-4339-acba-351223c5e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusionModel:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Define beta schedule\n",
    "        self.beta = torch.linspace(config.beta_start, config.beta_end, config.timesteps).to(self.device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.vae = VAE(config).to(self.device)\n",
    "        self.style_encoder = StyleEncoder(config).to(self.device)\n",
    "        self.content_encoder = ContentEncoder(config).to(self.device)\n",
    "        self.diffusion_model = DiffusionUNet(config).to(self.device)\n",
    "        \n",
    "        # Define optimizers\n",
    "        self.vae_optimizer = optim.Adam(self.vae.parameters(), lr=config.vae_lr)\n",
    "        self.style_optimizer = optim.Adam(self.style_encoder.parameters(), lr=config.diffusion_lr)\n",
    "        self.content_optimizer = optim.Adam(self.content_encoder.parameters(), lr=config.diffusion_lr)\n",
    "        self.diffusion_optimizer = optim.Adam(self.diffusion_model.parameters(), lr=config.diffusion_lr)\n",
    "    \n",
    "    def train_vae(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Train the VAE model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.vae_epochs\n",
    "        \n",
    "        print(\"Training VAE...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.vae.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                self.vae_optimizer.zero_grad()\n",
    "                \n",
    "                # Get handwriting images\n",
    "                images = batch['image'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "                \n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = recon_loss + 0.001 * kl_loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.vae_optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self._validate_vae(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self._save_checkpoint(f\"vae_epoch_{epoch+1}.pt\", model=self.vae)\n",
    "        \n",
    "        print(\"VAE training completed.\")\n",
    "    \n",
    "    def _validate_vae(self, val_loader):\n",
    "        \"\"\"Validate the VAE model\"\"\"\n",
    "        self.vae.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "                \n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = recon_loss + 0.001 * kl_loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        return val_loss / len(val_loader)\n",
    "    \n",
    "    def train_diffusion(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Train the diffusion model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.diffusion_epochs\n",
    "        \n",
    "        print(\"Training Diffusion Model...\")\n",
    "        \n",
    "        # Ensure VAE is in eval mode and frozen\n",
    "        self.vae.eval()\n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Calculate latent dimensions for clarity\n",
    "        latent_h = self.config.img_size[0] // 16  # Height divided by total downsampling factor\n",
    "        latent_w = self.config.img_size[1] // 16  # Width divided by total downsampling factor\n",
    "        latent_channels = 256  # Number of channels in the VAE bottleneck\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.diffusion_model.train()\n",
    "            self.style_encoder.train()\n",
    "            self.content_encoder.train()\n",
    "            \n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                # Zero gradients\n",
    "                self.diffusion_optimizer.zero_grad()\n",
    "                self.style_optimizer.zero_grad()\n",
    "                self.content_optimizer.zero_grad()\n",
    "                \n",
    "                # Get data\n",
    "                images = batch['image'].to(self.device)\n",
    "                text = batch['transcription']\n",
    "                \n",
    "                # Process text data - create proper tokenization\n",
    "                # For simplicity, using a fixed length and padding\n",
    "                max_seq_len = 100\n",
    "                text_indices = torch.zeros(len(text), max_seq_len).long().to(self.device)\n",
    "                for i, t in enumerate(text):\n",
    "                    # Convert string to character indices (simplified)\n",
    "                    # In production, use a proper tokenizer\n",
    "                    for j, char in enumerate(t[:max_seq_len]):\n",
    "                        text_indices[i, j] = ord(char) % 128  # Simple char to index mapping\n",
    "                \n",
    "                # Extract spatial latent representations from the VAE\n",
    "                with torch.no_grad():\n",
    "                    # Get encoder outputs\n",
    "                    encoder_output = self.vae.encoder(images)\n",
    "                    # Flatten for mu/logvar calculation\n",
    "                    encoder_flat = encoder_output.view(images.shape[0], -1)\n",
    "                    # Get latent distribution parameters\n",
    "                    mu = self.vae.fc_mu(encoder_flat)\n",
    "                    logvar = self.vae.fc_logvar(encoder_flat)\n",
    "                    # Sample from distribution\n",
    "                    z_flat = self.vae.reparameterize(mu, logvar)\n",
    "                    \n",
    "                    # Map back to spatial representation\n",
    "                    z_projected = self.vae.decoder_input(z_flat)\n",
    "                    # Reshape to spatial dimensions expected by diffusion model\n",
    "                    # [batch_size, channels, height, width]\n",
    "                    latent_spatial = z_projected.view(\n",
    "                        images.shape[0],\n",
    "                        latent_channels,\n",
    "                        latent_h,\n",
    "                        latent_w\n",
    "                    )\n",
    "                \n",
    "                # Sample random timesteps for each image\n",
    "                t = torch.randint(0, self.config.timesteps, (images.shape[0],), device=self.device)\n",
    "                \n",
    "                # Add noise to latent images\n",
    "                noise = torch.randn_like(latent_spatial)\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "                \n",
    "                # Calculate noisy latents\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                \n",
    "                # Encode style and content\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "                # Predict noise using the diffusion UNet\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "                # Compute loss between predicted and actual noise\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Apply gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.diffusion_model.parameters(), max_norm=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.style_encoder.parameters(), max_norm=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(self.content_encoder.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.diffusion_optimizer.step()\n",
    "                self.style_optimizer.step()\n",
    "                self.content_optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Calculate average training loss\n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation after each epoch\n",
    "            val_loss = self._validate_diffusion(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:  # Save more frequently\n",
    "                self._save_checkpoint(\n",
    "                    f\"diffusion_epoch_{epoch+1}.pt\", \n",
    "                    models=[self.diffusion_model, self.style_encoder, self.content_encoder]\n",
    "                )\n",
    "        \n",
    "        print(\"Diffusion model training completed!\")\n",
    "    \n",
    "    def _validate_diffusion(self, val_loader):\n",
    "        \"\"\"Validate the diffusion model\"\"\"\n",
    "        # Set models to evaluation mode\n",
    "        self.diffusion_model.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Calculate latent dimensions\n",
    "        latent_h = self.config.img_size[0] // 16\n",
    "        latent_w = self.config.img_size[1] // 16\n",
    "        latent_channels = 256\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Get data\n",
    "                images = batch['image'].to(self.device)\n",
    "                text = batch['transcription']\n",
    "                \n",
    "                # Process text data\n",
    "                max_seq_len = 100\n",
    "                text_indices = torch.zeros(len(text), max_seq_len).long().to(self.device)\n",
    "                for i, t in enumerate(text):\n",
    "                    for j, char in enumerate(t[:max_seq_len]):\n",
    "                        text_indices[i, j] = ord(char) % 128\n",
    "                \n",
    "                # Extract spatial latents from VAE\n",
    "                encoder_output = self.vae.encoder(images)\n",
    "                encoder_flat = encoder_output.view(images.shape[0], -1)\n",
    "                mu = self.vae.fc_mu(encoder_flat)\n",
    "                logvar = self.vae.fc_logvar(encoder_flat)\n",
    "                z_flat = self.vae.reparameterize(mu, logvar)\n",
    "                \n",
    "                # Map to spatial representation\n",
    "                z_projected = self.vae.decoder_input(z_flat)\n",
    "                latent_spatial = z_projected.view(\n",
    "                    images.shape[0], \n",
    "                    latent_channels,\n",
    "                    latent_h,\n",
    "                    latent_w\n",
    "                )\n",
    "                \n",
    "                # Sample random timesteps\n",
    "                t = torch.randint(0, self.config.timesteps, (images.shape[0],), device=self.device)\n",
    "                \n",
    "                # Add noise to latents\n",
    "                noise = torch.randn_like(latent_spatial)\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                \n",
    "                # Get style and content features\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        return val_loss / num_batches\n",
    "    \n",
    "    def _save_checkpoint(self, filename, model=None, models=None):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        save_path = os.path.join(self.config.save_dir, filename)\n",
    "        \n",
    "        if model is not None:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        elif models is not None:\n",
    "            save_dict = {}\n",
    "            for i, m in enumerate(models):\n",
    "                save_dict[f\"model_{i}\"] = m.state_dict()\n",
    "            torch.save(save_dict, save_path)\n",
    "    \n",
    "    def generate_handwriting(self, style_image, text, steps=None):\n",
    "        \"\"\"Generate handwriting with a given style and text\"\"\"\n",
    "        if steps is None:\n",
    "            steps = self.config.timesteps // 10  # Using fewer steps for inference can be faster\n",
    "        \n",
    "        # Ensure models are in eval mode\n",
    "        self.vae.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        self.diffusion_model.eval()\n",
    "        \n",
    "        # Calculate latent dimensions\n",
    "        latent_h = self.config.img_size[0] // 16\n",
    "        latent_w = self.config.img_size[1] // 16\n",
    "        latent_channels = 256\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Preprocess style image - make sure it has batch dimension\n",
    "            if isinstance(style_image, torch.Tensor):\n",
    "                # Add batch dimension if needed\n",
    "                if style_image.dim() == 3:\n",
    "                    style_image = style_image.unsqueeze(0)\n",
    "            else:\n",
    "                # Convert to tensor if needed (e.g., if it's a PIL image)\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize(self.config.img_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,))\n",
    "                ])\n",
    "                style_image = transform(style_image).unsqueeze(0)\n",
    "            \n",
    "            style_image = style_image.to(self.device)\n",
    "            \n",
    "            # Encode style - now expecting batch dimension correctly\n",
    "            style_features = self.style_encoder(style_image)\n",
    "            \n",
    "            # Process text input - always ensure batch dimension of 1\n",
    "            max_seq_len = 100\n",
    "            text_indices = torch.zeros(1, max_seq_len).long().to(self.device)\n",
    "            for i, char in enumerate(text[:max_seq_len]):\n",
    "                text_indices[0, i] = ord(char) % 128\n",
    "            \n",
    "            # Get content features\n",
    "            content_features = self.content_encoder(text_indices)\n",
    "            \n",
    "            # Start with random noise in the latent space (spatial format)\n",
    "            latent = torch.randn(1, latent_channels, latent_h, latent_w).to(self.device)\n",
    "            \n",
    "            # Reverse diffusion process\n",
    "            for i in tqdm(range(steps), desc=\"Generating handwriting\"):\n",
    "                # Get the timestep (counting backwards)\n",
    "                t = self.config.timesteps - i - 1\n",
    "                timestep = torch.tensor([t], device=self.device)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = self.diffusion_model(latent, timestep, style_features, content_features)\n",
    "                \n",
    "                # Get alpha values for current timestep\n",
    "                alpha = self.alpha[t]\n",
    "                alpha_cumprod = self.alpha_cumprod[t]\n",
    "                beta = self.beta[t]\n",
    "                \n",
    "                # No noise at timestep 0\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(latent)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(latent)\n",
    "                \n",
    "                # Update latent using the DDPM formula\n",
    "                latent = (1 / torch.sqrt(alpha)) * (\n",
    "                    latent - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * noise_pred\n",
    "                ) + torch.sqrt(beta) * noise\n",
    "            \n",
    "            # Reshape latent for the VAE decoder\n",
    "            # First reshape to appropriate format for VAE - THIS IS IMPORTANT\n",
    "            latent_flat = latent.reshape(1, -1)\n",
    "            \n",
    "            # Make sure the latent dimension matches what VAE expects\n",
    "            # Use the decoder_input layer to map from flattened spatial features to VAE latent dim\n",
    "            latent_for_decoder = self.vae.decoder_input(latent_flat[:, :self.config.latent_dim])\n",
    "            \n",
    "            # Reshape back to spatial format for the decoder\n",
    "            latent_spatial = latent_for_decoder.view(\n",
    "                1, \n",
    "                256,  # Match VAE's expected channel dimension\n",
    "                latent_h, \n",
    "                latent_w\n",
    "            )\n",
    "            \n",
    "            # Pass through VAE decoder\n",
    "            generated_image = self.vae.decoder(latent_spatial)\n",
    "            \n",
    "            # Convert to range [0, 1] for visualization\n",
    "            generated_image = (generated_image + 1) / 2\n",
    "            \n",
    "            return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3eea9d1c-92ad-4bbd-a349-5c07e8fc989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mac gpu\n"
     ]
    }
   ],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06baa2f5-1b48-4593-af90-338207dd6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrapper = IAMDatasetWrapper(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d74a560c-56ae-469f-8e19-614a1b1227df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IAM dataset...\n",
      "Dataset loaded. Train: 10682, Val: 1335, Test: 1336\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = data_wrapper.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "616186c4-312b-480e-9f90-329e92b47fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDiffusionModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "753a9e77-c738-47e0-8017-8f8c0c7672bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████████████████████████████████████████████████| 167/167 [00:31<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 0.2580, Val Loss: 0.1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████████████████████████████████████████████████| 167/167 [00:31<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 0.1023, Val Loss: 0.0940\n",
      "VAE training completed.\n"
     ]
    }
   ],
   "source": [
    "model.train_vae(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed28b430-e407-4f7f-bd28-fba1d1e8df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Diffusion Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:54<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 1.0028, Val Loss: 1.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 1.0006, Val Loss: 1.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:54<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 1.0003, Val Loss: 1.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:54<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 1.0004, Val Loss: 1.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████████████████████████████████████████████████| 167/167 [00:53<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 1.0005, Val Loss: 1.0010\n",
      "Diffusion model training completed!\n"
     ]
    }
   ],
   "source": [
    "model.train_diffusion(train_loader, val_loader) #mark-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79564054-92c7-427a-8c2f-76033fb0b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating handwriting: 100%|████████████████████████████████████████| 100/100 [00:00<00:00, 146.35it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, batch in enumerate(test_loader):\n",
    "    if i >= 1:  # Generate 5 samples\n",
    "        break\n",
    "    \n",
    "    # Get a sample for style reference\n",
    "    style_image = batch['image'][0]\n",
    "    text = batch['transcription'][0]\n",
    "    \n",
    "    # Generate new handwriting with the same style but different text\n",
    "    generated = model.generate_handwriting(style_image, \"This is a generated sample.\")\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(style_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Generated\")\n",
    "    plt.imshow(generated.squeeze().cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(config.results_dir, f\"sample_{i}.png\"))\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658fdc1-72e9-42c6-b0a0-84733c598638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
