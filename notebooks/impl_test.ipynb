{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c49434d-15c8-4a2c-82bf-eca4162aa5e8",
   "metadata": {},
   "source": [
    "# Implementation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d573b0c-6409-4602-b476-b99e59f6bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataset import IAMLinesDataset, IAMLinesDatasetPyTorch\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52e90d79-8790-4f99-bdcd-026e801d4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Dataset parameters\n",
    "        self.img_size = (64, 256)  # Height, Width - Adjusted for line-level segments\n",
    "        self.batch_size = 64\n",
    "        self.num_workers = 4\n",
    "        \n",
    "        # VAE parameters\n",
    "        self.latent_dim = 512  # Latent dimension for the VAE\n",
    "        self.vae_lr = 1e-4\n",
    "        self.vae_epochs = 2 #orig: 50\n",
    "        \n",
    "        # Diffusion model parameters\n",
    "        self.timesteps = 1000  # Number of diffusion steps\n",
    "        self.beta_start = 1e-4  # Starting noise schedule value\n",
    "        self.beta_end = 2e-2  # Ending noise schedule value\n",
    "        self.diffusion_lr = 1e-4\n",
    "        self.diffusion_epochs = 5 #orig: 100\n",
    "        \n",
    "        # Style and content encoder parameters\n",
    "        self.style_dim = 256\n",
    "        self.content_dim = 256\n",
    "        \n",
    "        # Training parameters\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "            print(\"Using mac gpu\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            print(\"Using cuda gpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Using cpu\")\n",
    "        self.save_dir = \"./models\"\n",
    "        self.results_dir = \"./results\"\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76bdaf49-cc48-4cc7-9909-4d4eb4fe1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and preprocessing\n",
    "class IAMDatasetWrapper:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(config.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Load the IAM dataset and create train/val/test splits\"\"\"\n",
    "        print(\"Loading IAM dataset...\")\n",
    "        # Assuming IAMLinesDatasetPyTorch is already implemented\n",
    "        data_path = 'data/lines.tgz' \n",
    "        xml_path = 'data/xml.tgz'    \n",
    "        iam_dataset = IAMLinesDataset(data_path, xml_path)\n",
    "        full_dataset = IAMLinesDatasetPyTorch(iam_dataset=iam_dataset, transform=self.transform)\n",
    "        \n",
    "        # Split into train/val/test\n",
    "        train_size = int(0.8 * len(full_dataset))\n",
    "        val_size = int(0.1 * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded. Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "94c105ea-c9f4-4fc5-a9c3-9ff51d300b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VAE, self).__init__()\n",
    "        self.config = config\n",
    "        self.latent_dim = config.latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the encoder output\n",
    "        h, w = config.img_size\n",
    "        self.encoder_output_size = (h // 16, w // 16, 256)\n",
    "        self.encoder_flattened_dim = self.encoder_output_size[0] * self.encoder_output_size[1] * self.encoder_output_size[2]\n",
    "        \n",
    "        # Mean and log variance layers\n",
    "        self.fc_mu = nn.Linear(self.encoder_flattened_dim, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.encoder_flattened_dim, self.latent_dim)\n",
    "        \n",
    "        # Decoder input layer\n",
    "        self.decoder_input = nn.Linear(self.latent_dim, self.encoder_flattened_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output in range [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1, 256, self.encoder_output_size[0], self.encoder_output_size[1])\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Style Encoder\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(StyleEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # Projection head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, config.style_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        style = self.fc(x)\n",
    "        return style\n",
    "\n",
    "# Content Encoder (for text)\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, config, vocab_size=128, embedding_dim=128):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Projection head\n",
    "        self.fc = nn.Linear(512, config.content_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Concatenate forward and backward hidden states\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Project to content dimension\n",
    "        content = self.fc(hidden)\n",
    "        return content\n",
    "\n",
    "# UNet model for diffusion\n",
    "class DiffusionUNet(nn.Module):\n",
    "    # def __init__(self, config):\n",
    "    #     super(DiffusionUNet, self).__init__()\n",
    "    #     self.config = config\n",
    "        \n",
    "    #     # Calculate latent image dimensions\n",
    "    #     h, w = config.img_size\n",
    "    #     self.latent_h, self.latent_w = h // 16, w // 16\n",
    "        \n",
    "    #     # Time embedding\n",
    "    #     self.time_embed = nn.Sequential(\n",
    "    #         nn.Linear(1, 128),\n",
    "    #         nn.SiLU(),\n",
    "    #         nn.Linear(128, 128),\n",
    "    #     )\n",
    "        \n",
    "    #     # Conditioning embeddings\n",
    "    #     self.style_embed = nn.Linear(config.style_dim, 128)\n",
    "    #     self.content_embed = nn.Linear(config.content_dim, 128)\n",
    "        \n",
    "    #     # Down blocks\n",
    "    #     self.down1 = self._make_down_block(256, 256)\n",
    "    #     self.down2 = self._make_down_block(256, 512)\n",
    "    #     self.down3 = self._make_down_block(512, 512)\n",
    "        \n",
    "    #     # Middle block\n",
    "    #     self.mid = nn.Sequential(\n",
    "    #         nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "    #         nn.GroupNorm(32, 512),\n",
    "    #         nn.SiLU(),\n",
    "    #         nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "    #         nn.GroupNorm(32, 512),\n",
    "    #         nn.SiLU(),\n",
    "    #     )\n",
    "        \n",
    "    #     # Up blocks\n",
    "    #     self.up1 = self._make_up_block(512, 512)\n",
    "    #     self.up2 = self._make_up_block(512, 256)\n",
    "    #     self.up3 = self._make_up_block(256, 256)\n",
    "        \n",
    "    #     # Output projection\n",
    "    #     self.out = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "    def __init__(self, config):\n",
    "        super(DiffusionUNet, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Calculate latent image dimensions\n",
    "        h, w = config.img_size\n",
    "        # Ensure latent dimensions match VAE bottleneck: H/16, W/16\n",
    "        self.latent_h, self.latent_w = h // 16, w // 16 \n",
    "        \n",
    "        # VAE bottleneck channels\n",
    "        vae_latent_channels = 256 # Should match the VAE's output channels before flattening\n",
    "        \n",
    "        # Conditioning embedding dimension (after combining time, style, content)\n",
    "        cond_channels = 128 # Based on time_embed, style_embed, content_embed outputs\n",
    "        \n",
    "        # --- Time and Conditioning Embeddings (Keep as is) ---\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "        self.style_embed = nn.Linear(config.style_dim, 128)\n",
    "        self.content_embed = nn.Linear(config.content_dim, 128)\n",
    "        \n",
    "        # --- ADD Initial Convolution Layer ---\n",
    "        # Input channels = VAE latent channels + conditioning channels\n",
    "        initial_in_channels = vae_latent_channels + cond_channels # 256 + 128 = 384\n",
    "        initial_out_channels = 256 # Desired channels for the first down block\n",
    "        self.initial_conv = nn.Conv2d(initial_in_channels, initial_out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # --- Down blocks ---\n",
    "        # Make sure the first down block starts with initial_out_channels\n",
    "        self.down1 = self._make_down_block(initial_out_channels, 256) \n",
    "        self.down2 = self._make_down_block(256, 512)\n",
    "        self.down3 = self._make_down_block(512, 512)\n",
    "        \n",
    "        # --- Middle block (Keep as is, input is 512 channels) ---\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        \n",
    "        # --- Up blocks ---\n",
    "        # Adjust input channels based on skip connections\n",
    "        self.up1 = self._make_up_block(512 + 512, 512) # mid output (512) + d3 skip (512)\n",
    "        self.up2 = self._make_up_block(512 + 512, 256) # u1 output (512) + d2 skip (512)\n",
    "        self.up3 = self._make_up_block(256 + 256, 256) # u2 output (256) + d1 skip (256)\n",
    "        \n",
    "        # --- Output projection ---\n",
    "        # Output should predict noise, matching the VAE latent channels\n",
    "        self.out = nn.Conv2d(256, vae_latent_channels, kernel_size=3, padding=1) \n",
    "\n",
    "    \n",
    "    def _make_down_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "    \n",
    "    def _make_up_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "    \n",
    "    # def forward(self, x, t, style, content):\n",
    "    #     # Reshape inputs\n",
    "    #     t = t.unsqueeze(-1).float()  # [B, 1]\n",
    "        \n",
    "    #     # Time embedding\n",
    "    #     t_emb = self.time_embed(t)  # [B, 128]\n",
    "        \n",
    "    #     # Style and content embeddings\n",
    "    #     style_emb = self.style_embed(style)  # [B, 128]\n",
    "    #     content_emb = self.content_embed(content)  # [B, 128]\n",
    "        \n",
    "    #     # Combine embeddings\n",
    "    #     cond_emb = (t_emb + style_emb + content_emb).unsqueeze(-1).unsqueeze(-1)  # [B, 128, 1, 1]\n",
    "    #     # cond_emb = cond_emb.expand(-1, -1, self.latent_h, self.latent_w)  # [B, 128, H, W]\n",
    "        \n",
    "    #     cond_emb = cond_emb.expand(-1, -1, x.shape[2], x.shape[3])\n",
    "    #     # _batch_size, _channels, H, W = x.shape \n",
    "\n",
    "    #     # Expand cond_emb to match the actual H and W of x\n",
    "    #     # cond_emb = cond_emb.expand(-1, -1, H, W) # NEW LINE\n",
    "    #     print(f\"x shape: {x.shape}, cond_emb shape: {cond_emb.shape}\") # mark-a\n",
    "        \n",
    "    #     # Initial projection\n",
    "    #     x = torch.cat([x, cond_emb], dim=1)  # [B, 256+128, H, W]\n",
    "        \n",
    "    #     # Encoder path\n",
    "    #     d1 = self.down1(x)\n",
    "    #     d2 = self.down2(d1)\n",
    "    #     d3 = self.down3(d2)\n",
    "        \n",
    "    #     # Middle\n",
    "    #     mid = self.mid(d3)\n",
    "        \n",
    "    #     # Decoder path with skip connections\n",
    "    #     u1 = self.up1(mid + d3)\n",
    "    #     u2 = self.up2(u1 + d2)\n",
    "    #     u3 = self.up3(u2 + d1)\n",
    "        \n",
    "    #     # Output\n",
    "    #     return self.out(u3)\n",
    "    def forward(self, x, t, style, content):\n",
    "        # x input shape expected: [B, vae_latent_channels, H', W'] e.g., [64, 256, 4, 16]\n",
    "        \n",
    "        t = t.unsqueeze(-1).float() # [B, 1]\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t) # [B, 128]\n",
    "        \n",
    "        # Style and content embeddings\n",
    "        style_emb = self.style_embed(style) # [B, 128]\n",
    "        content_emb = self.content_embed(content) # [B, 128]\n",
    "        \n",
    "        # Combine embeddings and expand to spatial dimensions\n",
    "        cond_emb = (t_emb + style_emb + content_emb).unsqueeze(-1).unsqueeze(-1) # [B, 128, 1, 1]\n",
    "        cond_emb = cond_emb.expand(-1, -1, self.latent_h, self.latent_w) # [B, 128, 4, 16]\n",
    "\n",
    "        # print(f\"x shape: {x.shape}, cond_emb shape: {cond_emb.shape}\") # Should now be [64, 256, 4, 16] and [64, 128, 4, 16]\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        x_cat = torch.cat([x, cond_emb], dim=1) # [B, 256+128, H', W'] -> [64, 384, 4, 16]\n",
    "        \n",
    "        # --- Apply Initial Convolution ---\n",
    "        h = self.initial_conv(x_cat) # Project channels: [64, 384, 4, 16] -> [64, 256, 4, 16]\n",
    "        \n",
    "        # Encoder path (pass projected 'h' to down1)\n",
    "        d1 = self.down1(h) # Input shape [64, 256, 4, 16]\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        \n",
    "        # Middle\n",
    "        mid = self.mid(d3)\n",
    "        \n",
    "        # Decoder path with skip connections - Ensure channel counts match in _make_up_block inputs\n",
    "        # Note: Skip connections should come from BEFORE the pooling in down blocks if using standard UNet blocks\n",
    "        # If _make_down_block applies pooling last, d1, d2, d3 have reduced spatial size but correct channels.\n",
    "        # If skip connections are added *before* upsampling in the up block, channel counts need adjustment.\n",
    "        # Assuming standard ResNet-like blocks in _make_down/up_block where skip happens before pooling/after upsampling:\n",
    "        # Let's assume skips d1, d2, d3 have shapes matching the outputs of down blocks *before pooling*\n",
    "        # And that u1, u2, u3 are outputs of up blocks *after convolution*.\n",
    "        # The provided _make_up_block applies Upsample first, then Conv.\n",
    "        # The provided _make_down_block applies Conv first, then Pool.\n",
    "        # Need to carefully handle skip connection shapes and channel dimensions.\n",
    "        # --> Re-evaluating the skip connection logic based on provided blocks:\n",
    "        # d1 output shape (after pool): [B, 256, 2, 8]\n",
    "        # d2 output shape (after pool): [B, 512, 1, 4]\n",
    "        # d3 output shape (after pool): [B, 512, H'/8, W'/8] -> error in block structure likely, should downsample H/W\n",
    "        # mid output shape: [B, 512, H'/8, W'/8]\n",
    "        # Let's assume down blocks work correctly spatially.\n",
    "        \n",
    "        # Simplified skip connections (concatenating output of upsample+conv with down block output):\n",
    "        # Check channel dimensions carefully based on your specific _make_up_block implementation\n",
    "        u1_input = torch.cat([mid, d3], dim=1) # Shape: [B, 512+512, H/8, W/8] - Requires up1 input channels 1024\n",
    "        u1 = self.up1(u1_input)           # Output shape: [B, 512, H/4, W/4]\n",
    "        \n",
    "        u2_input = torch.cat([u1, d2], dim=1)  # Shape: [B, 512+512, H/4, W/4] - Requires up2 input channels 1024\n",
    "        u2 = self.up2(u2_input)           # Output shape: [B, 256, H/2, W/2]\n",
    "        \n",
    "        u3_input = torch.cat([u2, d1], dim=1)  # Shape: [B, 256+256, H/2, W/2] - Requires up3 input channels 512\n",
    "        u3 = self.up3(u3_input)           # Output shape: [B, 256, H, W]\n",
    "\n",
    "        # Output - Predict noise matching VAE latent channels\n",
    "        return self.out(u3) # Output shape: [B, 256, 4, 16]\n",
    "\n",
    "# Diffusion Model\n",
    "class LatentDiffusionModel:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Define beta schedule\n",
    "        self.beta = torch.linspace(config.beta_start, config.beta_end, config.timesteps).to(self.device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.vae = VAE(config).to(self.device)\n",
    "        self.style_encoder = StyleEncoder(config).to(self.device)\n",
    "        self.content_encoder = ContentEncoder(config).to(self.device)\n",
    "        self.diffusion_model = DiffusionUNet(config).to(self.device)\n",
    "        \n",
    "        # Define optimizers\n",
    "        self.vae_optimizer = optim.Adam(self.vae.parameters(), lr=config.vae_lr)\n",
    "        self.style_optimizer = optim.Adam(self.style_encoder.parameters(), lr=config.diffusion_lr)\n",
    "        self.content_optimizer = optim.Adam(self.content_encoder.parameters(), lr=config.diffusion_lr)\n",
    "        self.diffusion_optimizer = optim.Adam(self.diffusion_model.parameters(), lr=config.diffusion_lr)\n",
    "    \n",
    "    def train_vae(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Train the VAE model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.vae_epochs\n",
    "        \n",
    "        print(\"Training VAE...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.vae.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                self.vae_optimizer.zero_grad()\n",
    "                \n",
    "                # Get handwriting images\n",
    "                images = batch['image'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "                \n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = recon_loss + 0.001 * kl_loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.vae_optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self._validate_vae(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self._save_checkpoint(f\"vae_epoch_{epoch+1}.pt\", model=self.vae)\n",
    "        \n",
    "        print(\"VAE training completed.\")\n",
    "    \n",
    "    def _validate_vae(self, val_loader):\n",
    "        \"\"\"Validate the VAE model\"\"\"\n",
    "        self.vae.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                recon_images, mu, logvar = self.vae(images)\n",
    "                \n",
    "                # Compute loss\n",
    "                recon_loss = F.mse_loss(recon_images, images)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = recon_loss + 0.001 * kl_loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        return val_loss / len(val_loader)\n",
    "    \n",
    "    # def train_diffusion(self, train_loader, val_loader, epochs=None):\n",
    "    #     \"\"\"Train the diffusion model\"\"\"\n",
    "    #     if epochs is None:\n",
    "    #         epochs = self.config.diffusion_epochs\n",
    "        \n",
    "    #     print(\"Training Diffusion Model...\")\n",
    "        \n",
    "    #     # Ensure VAE is in eval mode and frozen\n",
    "    #     self.vae.eval()\n",
    "    #     for param in self.vae.parameters():\n",
    "    #         param.requires_grad = False\n",
    "        \n",
    "    #     for epoch in range(epochs):\n",
    "    #         # Training\n",
    "    #         self.diffusion_model.train()\n",
    "    #         self.style_encoder.train()\n",
    "    #         self.content_encoder.train()\n",
    "            \n",
    "    #         train_loss = 0\n",
    "            \n",
    "    #         for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "    #             self.diffusion_optimizer.zero_grad()\n",
    "    #             self.style_optimizer.zero_grad()\n",
    "    #             self.content_optimizer.zero_grad()\n",
    "                \n",
    "    #             # Get data\n",
    "    #             images = batch['image'].to(self.device)\n",
    "    #             text = batch['transcription']  # This would need preprocessing\n",
    "                \n",
    "    #             # Encode text to indices (simplified - you'd need proper text tokenization)\n",
    "    #             text_indices = torch.zeros(len(text), 100).long().to(self.device)\n",
    "                \n",
    "    #             # Encode images to latent space using VAE\n",
    "    #             with torch.no_grad():\n",
    "    #                 mu, _ = self.vae.encode(images)\n",
    "    #                 latent_images = mu\n",
    "    #                 # latent_images = mu.reshape(-1, 256, 4, 16) #change-c-1\n",
    "                \n",
    "    #             # Encode style and content\n",
    "    #             style_features = self.style_encoder(images)\n",
    "    #             content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "    #             # Sample a random timestep for each image\n",
    "    #             t = torch.randint(0, self.config.timesteps, (images.shape[0],)).to(self.device)\n",
    "                \n",
    "    #             # Add noise to latent images\n",
    "    #             noise = torch.randn_like(latent_images)\n",
    "    #             alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "    #             noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_images + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                \n",
    "    #             # Predict the noise\n",
    "    #             noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "    #             # Compute loss\n",
    "    #             loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "    #             # Backward pass\n",
    "    #             loss.backward()\n",
    "    #             self.diffusion_optimizer.step()\n",
    "    #             self.style_optimizer.step()\n",
    "    #             self.content_optimizer.step()\n",
    "                \n",
    "    #             train_loss += loss.item()\n",
    "            \n",
    "    #         train_loss /= len(train_loader)\n",
    "            \n",
    "    #         # Validation\n",
    "    #         val_loss = self._validate_diffusion(val_loader)\n",
    "            \n",
    "    #         print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "    #         # Save checkpoint\n",
    "    #         if (epoch + 1) % 10 == 0:\n",
    "    #             self._save_checkpoint(\n",
    "    #                 f\"diffusion_epoch_{epoch+1}.pt\", \n",
    "    #                 models=[self.diffusion_model, self.style_encoder, self.content_encoder]\n",
    "    #             )\n",
    "        \n",
    "    #     print(\"Diffusion model training completed.\")\n",
    "\n",
    "    def train_diffusion(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Train the diffusion model\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config.diffusion_epochs\n",
    "        \n",
    "        print(\"Training Diffusion Model...\")\n",
    "        \n",
    "        # Ensure VAE is in eval mode and frozen\n",
    "        self.vae.eval()\n",
    "        for param in self.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.diffusion_model.train()\n",
    "            self.style_encoder.train()\n",
    "            self.content_encoder.train()\n",
    "            \n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                self.diffusion_optimizer.zero_grad()\n",
    "                self.style_optimizer.zero_grad()\n",
    "                self.content_optimizer.zero_grad()\n",
    "                \n",
    "                # Get data\n",
    "                images = batch['image'].to(self.device)\n",
    "                text = batch['transcription']  # This would need preprocessing\n",
    "                \n",
    "                # Encode text to indices (simplified - you'd need proper text tokenization)\n",
    "                text_indices = torch.zeros(len(text), 100).long().to(self.device)\n",
    "                \n",
    "                # Encode images to latent space using VAE\n",
    "                with torch.no_grad():\n",
    "                    # Get mean and log variance from the encoder\n",
    "                    mu, logvar = self.vae.encode(images) \n",
    "                    # Reparameterize to get the latent vector z (flattened)\n",
    "                    z_flat = self.vae.reparameterize(mu, logvar) \n",
    "                    # Project and reshape z back to spatial latent dimensions [B, C, H', W']\n",
    "                    # (Mimicking the start of the VAE's decode method)\n",
    "                    latent_spatial = self.vae.decoder_input(z_flat)\n",
    "                    latent_spatial = latent_spatial.view(\n",
    "                        -1,  # Batch size\n",
    "                        256, # VAE bottleneck channels\n",
    "                        self.config.img_size[0] // 16, # Latent Height (4)\n",
    "                        self.config.img_size[1] // 16  # Latent Width (16) - Assuming img_width is 256\n",
    "                    ) # Expected shape: [64, 256, 4, 16]\n",
    "    \n",
    "                # Sample a random timestep for each image\n",
    "                t = torch.randint(0, self.config.timesteps, (images.shape[0],)).to(self.device)\n",
    "                \n",
    "                # Add noise to the *spatial* latent images\n",
    "                noise = torch.randn_like(latent_spatial) # Noise shape: [64, 256, 4, 16]\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1) # Shape: [64, 1, 1, 1]\n",
    "                \n",
    "                # Calculate noisy latents correctly using the spatial representation\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_spatial + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                # noisy_latents expected shape: [64, 256, 4, 16]\n",
    "                \n",
    "                # Encode style and content (Keep this part as is)\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "                # Predict the noise using the Diffusion UNet\n",
    "                # Pass the correctly shaped noisy_latents\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "                # Compute loss (noise_pred should also have shape [64, 256, 4, 16])\n",
    "                loss = F.mse_loss(noise_pred, noise) # Match between predicted and actual noise\n",
    "                \n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.diffusion_optimizer.step()\n",
    "                self.style_optimizer.step()\n",
    "                self.content_optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self._validate_diffusion(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self._save_checkpoint(\n",
    "                    f\"diffusion_epoch_{epoch+1}.pt\", \n",
    "                    models=[self.diffusion_model, self.style_encoder, self.content_encoder]\n",
    "                )\n",
    "        \n",
    "        print(\"Diffusion model training completed.\")\n",
    "    \n",
    "    def _validate_diffusion(self, val_loader):\n",
    "        \"\"\"Validate the diffusion model\"\"\"\n",
    "        self.diffusion_model.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Get data\n",
    "                images = batch['image'].to(self.device)\n",
    "                text = batch['transcription']\n",
    "                \n",
    "                # Encode text to indices (simplified)\n",
    "                text_indices = torch.zeros(len(text), 100).long().to(self.device)\n",
    "                \n",
    "                # Encode images to latent space using VAE\n",
    "                mu, _ = self.vae.encode(images)\n",
    "                latent_images = mu\n",
    "                \n",
    "                # Encode style and content\n",
    "                style_features = self.style_encoder(images)\n",
    "                content_features = self.content_encoder(text_indices)\n",
    "                \n",
    "                # Sample a random timestep for each image\n",
    "                t = torch.randint(0, self.config.timesteps, (images.shape[0],)).to(self.device)\n",
    "                \n",
    "                # Add noise to latent images\n",
    "                noise = torch.randn_like(latent_images)\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "                noisy_latents = torch.sqrt(alpha_cumprod_t) * latent_images + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                \n",
    "                # Predict the noise\n",
    "                noise_pred = self.diffusion_model(noisy_latents, t, style_features, content_features)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        return val_loss / len(val_loader)\n",
    "    \n",
    "    def _save_checkpoint(self, filename, model=None, models=None):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        save_path = os.path.join(self.config.save_dir, filename)\n",
    "        \n",
    "        if model is not None:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        elif models is not None:\n",
    "            save_dict = {}\n",
    "            for i, m in enumerate(models):\n",
    "                save_dict[f\"model_{i}\"] = m.state_dict()\n",
    "            torch.save(save_dict, save_path)\n",
    "    \n",
    "    def generate_handwriting(self, style_image, text, steps=50):\n",
    "        \"\"\"Generate handwriting with a given style and text\"\"\"\n",
    "        # Ensure models are in eval mode\n",
    "        self.vae.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        self.diffusion_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Preprocess style image\n",
    "            style_image = style_image.unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Encode style\n",
    "            style_features = self.style_encoder(style_image)\n",
    "            \n",
    "            # Encode text (simplified)\n",
    "            text_indices = torch.zeros(1, 100).long().to(self.device)  # Placeholder\n",
    "            content_features = self.content_encoder(text_indices)\n",
    "            \n",
    "            # Start with random noise in the latent space\n",
    "            latent_shape = (1, 256, self.config.img_size[0] // 16, self.config.img_size[1] // 16)\n",
    "            latent = torch.randn(latent_shape).to(self.device)\n",
    "            \n",
    "            # Denoise gradually\n",
    "            for t in tqdm(range(self.config.timesteps - 1, -1, -1), desc=\"Generating\"):\n",
    "                # Get the timestep\n",
    "                timestep = torch.tensor([t], device=self.device)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = self.diffusion_model(latent, timestep, style_features, content_features)\n",
    "                \n",
    "                # Get alpha values for current timestep\n",
    "                alpha = self.alpha[t]\n",
    "                alpha_cumprod = self.alpha_cumprod[t]\n",
    "                beta = self.beta[t]\n",
    "                \n",
    "                # No noise at timestep 0\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(latent)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(latent)\n",
    "                \n",
    "                # Update latent\n",
    "                latent = (1 / torch.sqrt(alpha)) * (\n",
    "                    latent - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * noise_pred\n",
    "                ) + torch.sqrt(beta) * noise\n",
    "            \n",
    "            # Decode latent to image\n",
    "            generated_image = self.vae.decode(latent)\n",
    "            \n",
    "            return generated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3eea9d1c-92ad-4bbd-a349-5c07e8fc989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mac gpu\n"
     ]
    }
   ],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "06baa2f5-1b48-4593-af90-338207dd6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrapper = IAMDatasetWrapper(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d74a560c-56ae-469f-8e19-614a1b1227df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IAM dataset...\n",
      "Dataset loaded. Train: 10682, Val: 1335, Test: 1336\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = data_wrapper.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "616186c4-312b-480e-9f90-329e92b47fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDiffusionModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "753a9e77-c738-47e0-8017-8f8c0c7672bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████████████████████████████████████████████████| 167/167 [00:31<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 0.4188, Val Loss: 0.1309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████████████████████████████████████████████████| 167/167 [00:31<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 0.1192, Val Loss: 0.1092\n",
      "VAE training completed.\n"
     ]
    }
   ],
   "source": [
    "model.train_vae(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed28b430-e407-4f7f-bd28-fba1d1e8df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Diffusion Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|                                                                | 0/167 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x1x4). Calculated output size: (512x0x2). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#mark-b\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[106], line 598\u001b[0m, in \u001b[0;36mLatentDiffusionModel.train_diffusion\u001b[0;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m    594\u001b[0m content_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_encoder(text_indices)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# Predict the noise using the Diffusion UNet\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Pass the correctly shaped noisy_latents\u001b[39;00m\n\u001b[0;32m--> 598\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# Compute loss (noise_pred should also have shape [64, 256, 4, 16])\u001b[39;00m\n\u001b[1;32m    601\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(noise_pred, noise) \u001b[38;5;66;03m# Match between predicted and actual noise\u001b[39;00m\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[106], line 329\u001b[0m, in \u001b[0;36mDiffusionUNet.forward\u001b[0;34m(self, x, t, style, content)\u001b[0m\n\u001b[1;32m    327\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1(h) \u001b[38;5;66;03m# Input shape [64, 256, 4, 16]\u001b[39;00m\n\u001b[1;32m    328\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(d1)\n\u001b[0;32m--> 329\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown3\u001b[49m\u001b[43m(\u001b[49m\u001b[43md2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Middle\u001b[39;00m\n\u001b[1;32m    332\u001b[0m mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid(d3)\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/bin/virtualenvs/cpsc440/lib/python3.13/site-packages/torch/nn/modules/pooling.py:756\u001b[0m, in \u001b[0;36mAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisor_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x1x4). Calculated output size: (512x0x2). Output size is too small"
     ]
    }
   ],
   "source": [
    "model.train_diffusion(train_loader, val_loader) #mark-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec56b2ea-aa04-479f-99d4-b11eb849b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # Then train diffusion model\n",
    "    model.train_diffusion(train_loader, val_loader)\n",
    "    \n",
    "    # Generate some samples\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= 5:  # Generate 5 samples\n",
    "            break\n",
    "        \n",
    "        # Get a sample for style reference\n",
    "        style_image = batch['image'][0]\n",
    "        text = batch['transcription'][0]\n",
    "        \n",
    "        # Generate new handwriting with the same style but different text\n",
    "        generated = model.generate_handwriting(style_image, \"This is a generated sample.\")\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Original\")\n",
    "        plt.imshow(style_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Generated\")\n",
    "        plt.imshow(generated.squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.savefig(os.path.join(config.results_dir, f\"sample_{i}.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22552b-5c70-4a92-9794-d1a40a8ca079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
